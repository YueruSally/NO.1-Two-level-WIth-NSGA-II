#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Two-Level NSGA-II for Multi-batch Multimodal Transportation
- Upper level: NSGA-II optimises an OD→route-portfolio (batch-agnostic strategic layer)
- Lower level: Greedy priority-based batch allocation + feasibility screening (capacity/timetable/time-window)
               with bottleneck feedback to guide upper-level mutation; no exact optimisation here.
- Evaluation: Uses MultimodalOptimizationModel's three objectives (cost/emissions/makespan).
- Data: read from extended.xlsx (nodes/arcs/timetable).
- Output: optimisation_results.txt (Pareto overview + detailed split report + decomposed routes)
"""

import os
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass, field
from enum import IntEnum
import random
import copy
from collections import defaultdict, deque
import math
import re

# =========================
# Symbols and data classes (Table II alignment)
# =========================

class Mode(IntEnum):
    """Transport modes M = {1: road, 2: rail, 3: waterway}"""
    ROAD = 1
    RAIL = 2
    WATER = 3

@dataclass
class NetworkParameters:
    """Parameters matching Table II notation"""
    L_ij: Dict[Tuple[str, str], float] = field(default_factory=dict)
    CAP_node: Dict[str, float] = field(default_factory=dict)       # TEU/h (if TEU/day in source, divide by 24 on read)
    CAP_border: Dict[str, float] = field(default_factory=dict)
    rho: Dict[str, str] = field(default_factory=dict)
    M: float = 1e6
    I_border: Dict[Tuple[str, str], bool] = field(default_factory=dict)
    B_j: Dict[str, bool] = field(default_factory=dict)
    C_ijm: Dict[Tuple[str, str, Mode], float] = field(default_factory=dict)
    TC_jmn: Dict[Tuple[str, Mode, Mode], float] = field(default_factory=dict)
    W_hold: Dict[str, float] = field(default_factory=dict)
    W_proc: Dict[str, float] = field(default_factory=dict)
    W_cong: Dict[str, float] = field(default_factory=dict)
    P_k: Dict[int, float] = field(default_factory=dict)
    CT_r: Dict[str, float] = field(default_factory=dict)
    Theta_rm: Dict[Tuple[str, Mode], float] = field(default_factory=dict)
    E_mr: Dict[Tuple[Mode, str], float] = field(default_factory=dict)
    V_m: Dict[Mode, float] = field(default_factory=dict)
    TT_jmn: Dict[Tuple[str, Mode, Mode], float] = field(default_factory=dict)
    BD_j: Dict[str, float] = field(default_factory=dict)
    CC_j: Dict[str, float] = field(default_factory=dict)
    S_jm: Dict[Tuple[str, Mode], List[float]] = field(default_factory=dict)   # departure times (hours, 0–168)
    arc_modes: Dict[Tuple[str, str], List[Mode]] = field(default_factory=dict)

@dataclass
class Batch:
    """Batch k ∈ K"""
    id: int
    O_k: str
    DEST_k: str
    Q_k: float
    ET_k: float
    LT_k: float

@dataclass
class DecisionVariables:
    """Primary decision variables"""
    f_ijmk: Dict[Tuple[str, str, Mode, int], float] = field(default_factory=dict)
    z_ijmk: Dict[Tuple[str, str, Mode, int], float] = field(default_factory=dict)

@dataclass
class AuxiliaryVariables:
    """Auxiliary variables"""
    y_jmnk: Dict[Tuple[str, Mode, Mode, int], float] = field(default_factory=dict)
    t_max: float = 0.0
    a_jk: Dict[Tuple[str, int], float] = field(default_factory=dict)  # arrival time at node j
    d_jk: Dict[Tuple[str, int], float] = field(default_factory=dict)  # departure time at node j
    q_jmk: Dict[Tuple[str, Mode, int], float] = field(default_factory=dict)
    w_jk: Dict[Tuple[str, int], float] = field(default_factory=dict)
    p_jk: Dict[Tuple[str, int], float] = field(default_factory=dict)
    delta_k: Dict[int, float] = field(default_factory=dict)

# ===== Model used for objective evaluation =====

class MultimodalOptimizationModel:
    """Model to evaluate three objectives (cost, emissions, makespan)"""
    def __init__(self, params: NetworkParameters, network: dict, batches: List[Batch]):
        self.params = params
        self.network = network
        self.batches = batches
        self.nodes = set(network['nodes'].keys())
        self.arcs = network['arcs']

    def determine_transshipment_volumes(self, decision_vars: DecisionVariables) -> Dict:
        """Compute y_jmnk from flows: amount turned from mode m to n at node j for batch k"""
        y_jmnk = {}
        for batch in self.batches:
            k = batch.id
            for j in self.nodes:
                if j == batch.O_k or j == batch.DEST_k:
                    continue
                incoming = defaultdict(float)
                outgoing = defaultdict(float)
                for (i, j2), arc_data in self.arcs.items():
                    if j2 == j:
                        for m in arc_data.get('modes', []):
                            key = (i, j, m, k)
                            if key in decision_vars.f_ijmk:
                                incoming[m] += decision_vars.f_ijmk[key]
                    if i == j:
                        for m in arc_data.get('modes', []):
                            key = (j, j2, m, k)
                            if key in decision_vars.f_ijmk:
                                outgoing[m] += decision_vars.f_ijmk[key]
                for m in Mode:
                    for n in Mode:
                        if m != n:
                            y_val = min(incoming[m], outgoing[n])
                            if y_val > 0:
                                y_jmnk[(j, m, n, k)] = y_val
        return y_jmnk

    def calculate_objective_1(self, decision_vars: DecisionVariables, aux_vars: AuxiliaryVariables) -> float:
        """Total cost"""
        total_cost = 0.0
        for batch in self.batches:
            k = batch.id
            # transport cost
            transport_cost = 0.0
            for (i, j), arc_data in self.arcs.items():
                for m in arc_data.get('modes', []):
                    key = (i, j, m, k)
                    if key in decision_vars.f_ijmk:
                        distance = arc_data.get('distances', {}).get(m, 0.0)
                        unit_cost = self.params.C_ijm.get((i, j, m), 1.5)
                        transport_cost += unit_cost * distance * decision_vars.f_ijmk[key]
            # transhipment cost
            transship_cost = 0.0
            for (j, m, n, k2), y_value in aux_vars.y_jmnk.items():
                if k2 == k and m != n:
                    tc = self.params.TC_jmn.get((j, m, n), 50.0)
                    transship_cost += tc * y_value
            # carbon cost
            carbon_cost = 0.0
            for (i, j), arc_data in self.arcs.items():
                for m in arc_data.get('modes', []):
                    key = (i, j, m, k)
                    if key in decision_vars.f_ijmk:
                        region = self.params.rho.get(i, 'CN')
                        ct = self.params.CT_r.get(region, 10.0)
                        theta = self.params.Theta_rm.get((region, m), 1.0)
                        ef = self.params.E_mr.get((m, region), 0.05)
                        distance = arc_data.get('distances', {}).get(m, 0.0)
                        carbon_cost += ct * theta * ef * distance * decision_vars.f_ijmk[key]
            # waiting/processing/congestion
            waiting_cost = 0.0
            for (j, k2), w_time in aux_vars.w_jk.items():
                if k2 == k:
                    total_flow_at_j = sum(
                        decision_vars.f_ijmk.get((i, j, m, k), 0.0)
                        for (i, j2), arc_data in self.arcs.items() if j2 == j
                        for m in arc_data.get('modes', [])
                    )
                    w_hold = self.params.W_hold.get(j, 2.0)
                    waiting_cost += w_hold * total_flow_at_j * w_time
            for (j, k2), p_time in aux_vars.p_jk.items():
                if k2 == k:
                    total_flow_at_j = sum(
                        decision_vars.f_ijmk.get((i, j, m, k), 0.0)
                        for (i, j2), arc_data in self.arcs.items() if j2 == j
                        for m in arc_data.get('modes', [])
                    )
                    w_proc = self.params.W_proc.get(j, 5.0)
                    waiting_cost += w_proc * total_flow_at_j * p_time
            for (j, m, k2), q_time in aux_vars.q_jmk.items():
                if k2 == k:
                    total_flow = sum(
                        decision_vars.f_ijmk.get((i, j, m, k), 0.0)
                        for (i, j2), _ in self.arcs.items() if j2 == j
                    )
                    w_cong = self.params.W_cong.get(j, 10.0)
                    waiting_cost += w_cong * total_flow * q_time
            # tardiness
            delta = aux_vars.delta_k.get(k, 0.0)
            p_k = self.params.P_k.get(k, 20.0)
            tardiness_cost = p_k * batch.Q_k * delta
            total_cost += transport_cost + transship_cost + carbon_cost + waiting_cost + tardiness_cost
        return total_cost

    def calculate_objective_2(self, decision_vars: DecisionVariables) -> float:
        """Total carbon emissions"""
        total_emissions = 0.0
        for batch in self.batches:
            k = batch.id
            for (i, j), arc_data in self.arcs.items():
                for m in arc_data.get('modes', []):
                    key = (i, j, m, k)
                    if key in decision_vars.f_ijmk:
                        region = self.params.rho.get(i, 'CN')
                        ef = self.params.E_mr.get((m, region), 0.05)
                        distance = arc_data.get('distances', {}).get(m, 0.0)
                        total_emissions += ef * distance * decision_vars.f_ijmk[key]
        return total_emissions

    def calculate_objective_3(self, aux_vars: AuxiliaryVariables) -> float:
        """Makespan"""
        return aux_vars.t_max

# =========================
# Two-level framework structures
# =========================

@dataclass
class RouteSegment:
    nodes: List[str]       # e.g. ["Xi'an", "Khorgos", ..., "Berlin"]
    modes: List[Mode]      # len = len(nodes) - 1

@dataclass
class PortfolioChromosome:
    """Upper-level chromosome: for each (O,D) store a small pool of candidate RouteSegment"""
    route_pool: Dict[Tuple[str, str], List[RouteSegment]] = field(default_factory=dict)

@dataclass
class AllocationResult:
    feasible: bool
    dv: DecisionVariables
    aux: AuxiliaryVariables
    objectives: Tuple[float, float, float]
    bottleneck_nodes: List[str] = field(default_factory=list)
    late_batches: List[int] = field(default_factory=list)

# =========================
# Lower-level: greedy allocation & feasibility screening
# =========================

class LowerLevelAllocator:
    """
    Given an OD→route-portfolio, allocate batches in priority order (tight LT, larger Q first),
    check approximate node capacities in a rolling 168h horizon, and note timetable presence.
    If infeasible, return bottleneck nodes to guide upper-level mutation. No exact optimisation is attempted.
    """
    def __init__(self, model: MultimodalOptimizationModel, plan_horizon_h: float = 168.0):
        self.model = model
        self.params = model.params
        self.network = model.network
        self.plan_horizon_h = plan_horizon_h  # hours

    def evaluate(self, chrom: PortfolioChromosome) -> AllocationResult:
        dv = DecisionVariables()
        aux = AuxiliaryVariables()

        # Priority: (LT-ET) ascending, then -Q
        batches_sorted = sorted(
            self.model.batches,
            key=lambda b: (b.LT_k - b.ET_k, -b.Q_k)
        )

        node_load: Dict[str, float] = defaultdict(float)  # TEU aggregated within plan_horizon
        bottlenecks: Set[str] = set()
        late_batches: List[int] = []

        for b in batches_sorted:
            key = (b.O_k, b.DEST_k)
            cand_routes = chrom.route_pool.get(key, [])
            if not cand_routes:
                bottlenecks.add(b.O_k)
                continue

            # Prefer shorter, non-bottleneck, with fewer rail/water switches
            def score_route(r: RouteSegment) -> float:
                penalty = sum(1.0 for n in r.nodes if n in bottlenecks)
                length = self._route_distance(r)
                rail_water = sum(1 for m in r.modes if m in (Mode.RAIL, Mode.WATER))
                return length + 100 * penalty + 5 * rail_water

            routes_sorted = sorted(cand_routes, key=score_route)

            parts: List[Tuple[RouteSegment, float]] = []
            remaining = b.Q_k

            for route in routes_sorted:
                if remaining <= 1e-6:
                    break
                cap_ok, min_headroom, viol_nodes = self._check_capacity_margin(route, remaining, node_load)
                if not cap_ok:
                    if min_headroom > 1e-6:
                        use = max(0.0, 0.8 * min_headroom)
                        if use > 1e-6:
                            parts.append((route, use))
                            self._accumulate_node_load(route, use, node_load)
                            remaining -= use
                            bottlenecks.update(viol_nodes)
                    else:
                        bottlenecks.update(viol_nodes)
                    continue
                # timetable presence (non-blocking)
                sched_missing = self._check_timetable_presence(route)
                if sched_missing:
                    bottlenecks.update(sched_missing)
                # allocate on this route
                use = min(remaining, b.Q_k)
                parts.append((route, use))
                self._accumulate_node_load(route, use, node_load)
                remaining -= use

            if remaining > 1e-6:
                # not fully allocated -> mark the first route's nodes as bottlenecks
                if routes_sorted:
                    for n in routes_sorted[0].nodes:
                        bottlenecks.add(n)
                continue

            # inject flows into dv
            for route, flow in parts:
                self._inject_route_flow_to_dv(b.id, route, flow, dv)

        # If nothing allocated at all
        if len(dv.f_ijmk) == 0:
            aux = AuxiliaryVariables()
            return AllocationResult(
                feasible=False, dv=dv, aux=aux,
                objectives=(1e15, 1e12, 1e9),
                bottleneck_nodes=sorted(bottlenecks),
                late_batches=[]
            )

        # time variables (light) and objectives
        aux.y_jmnk = self.model.determine_transshipment_volumes(dv)
        self._calculate_temporal_variables_light(dv, aux)

        # tardiness marks
        for b in self.model.batches:
            arrive = 0.0
            for (j_k, k), t in aux.a_jk.items():
                if k == b.id and j_k == b.DEST_k:
                    arrive = max(arrive, t)
            if arrive - b.LT_k > 1e-6:
                late_batches.append(b.id)

        obj1 = self.model.calculate_objective_1(dv, aux)
        obj2 = self.model.calculate_objective_2(dv)
        obj3 = self.model.calculate_objective_3(aux)

        feasible = len(bottlenecks) == 0
        return AllocationResult(
            feasible=feasible,
            dv=dv,
            aux=aux,
            objectives=(obj1, obj2, obj3),
            bottleneck_nodes=sorted(bottlenecks),
            late_batches=late_batches
        )

    # ---- helpers ----

    def _route_distance(self, r: RouteSegment) -> float:
        d = 0.0
        for i in range(len(r.nodes) - 1):
            a, b = r.nodes[i], r.nodes[i + 1]
            m = r.modes[i]
            arc = self.network['arcs'].get((a, b), {})
            d += arc.get('distances', {}).get(m, 0.0)
        return d

    def _check_capacity_margin(self, r: RouteSegment, flow: float,
                               node_load: Dict[str, float]) -> Tuple[bool, float, List[str]]:
        """
        Approximate capacity check within the rolling horizon:
        Sum_throughput(node) ≤ CAP_node[node] * plan_horizon_h
        Returns: (is_feasible_now, min_headroom, violating_nodes)
        """
        violating = []
        headrooms = []
        for j in r.nodes:
            cap_h = self.params.CAP_node.get(j, 1e9)
            limit = cap_h * self.plan_horizon_h
            used = node_load.get(j, 0.0)
            headroom = max(0.0, limit - used)
            headrooms.append(headroom)
            if used + flow > limit + 1e-6:
                violating.append(j)
        if violating:
            return False, (min(headrooms) if headrooms else 0.0), violating
        return True, (min(headrooms) if headrooms else 1e9), []

    def _accumulate_node_load(self, r: RouteSegment, flow: float, node_load: Dict[str, float]):
        for j in r.nodes:
            node_load[j] += flow

    def _check_timetable_presence(self, r: RouteSegment) -> List[str]:
        """For RAIL/WATER: if node lacks departure timetable for the mode, flag as potential bottleneck."""
        missing = []
        for i in range(len(r.nodes) - 1):
            j = r.nodes[i]
            m = r.modes[i]
            if m in (Mode.RAIL, Mode.WATER):
                if (j, m) not in self.params.S_jm:
                    missing.append(j)
        return missing

    def _inject_route_flow_to_dv(self, k: int, r: RouteSegment, flow: float, dv: DecisionVariables):
        for i in range(len(r.nodes) - 1):
            a, b = r.nodes[i], r.nodes[i + 1]
            m = r.modes[i]
            key = (a, b, m, k)
            dv.f_ijmk[key] = dv.f_ijmk.get(key, 0.0) + flow
            dv.z_ijmk[key] = 1.0

    def _calculate_temporal_variables_light(self, dv: DecisionVariables, aux: AuxiliaryVariables):
        """
        Lightweight timing:
        - ROAD: depart immediately
        - RAIL/WATER: depart at earliest feasible scheduled time if available; otherwise immediately
        - Handling at transhipment/border: use max of TT_jmn, add BD_j/CC_j if border
        """
        params = self.params
        arcs = self.network['arcs']
        for b in self.model.batches:
            k = b.id
            aux.a_jk[(b.O_k, k)] = b.ET_k
            dest_arrival = 0.0

            # collect arcs for this batch
            graph = defaultdict(list)
            for (i, j, m, k2), flow in dv.f_ijmk.items():
                if k2 == k and flow > 0:
                    graph[i].append((j, m, flow))
            q = deque([(b.O_k, b.ET_k)])
            visited = set([b.O_k])

            while q:
                node, arrive_time_at_node = q.popleft()
                aux.a_jk[(node, k)] = max(aux.a_jk.get((node, k), 0.0), arrive_time_at_node)

                # processing time
                process_time = 0.0
                out_modes = set(m for (_, m, _) in graph.get(node, []))
                if len(out_modes) >= 2:
                    process_time = max(
                        [params.TT_jmn.get((node, m1, m2), 0.0)
                         for m1 in out_modes for m2 in out_modes if m1 != m2] + [0.0]
                    )
                if params.B_j.get(node, False):
                    process_time += params.BD_j.get(node, 0.0) + params.CC_j.get(node, 0.0)

                for (nxt, m, flow) in graph.get(node, []):
                    # departure time
                    if m in (Mode.RAIL, Mode.WATER):
                        schedules = params.S_jm.get((node, m), [])
                        ready_time = aux.a_jk[(node, k)] + process_time
                        if schedules:
                            mod = ready_time % 168
                            valid = [s for s in schedules if s >= mod]
                            if valid:
                                dep = ready_time - mod + min(valid)
                            else:
                                dep = ready_time - mod + 168 + schedules[0]
                        else:
                            dep = ready_time
                    else:
                        dep = aux.a_jk[(node, k)] + process_time

                    aux.d_jk[(node, k)] = dep
                    arc = arcs.get((node, nxt), {})
                    dist = arc.get('distances', {}).get(m, 0.0)
                    speed = params.V_m.get(m, 50.0)
                    travel = dist / speed if speed > 0 else 0.0
                    arrive_next = dep + travel
                    aux.a_jk[(nxt, k)] = max(aux.a_jk.get((nxt, k), 0.0), arrive_next)

                    if nxt == b.DEST_k:
                        dest_arrival = max(dest_arrival, arrive_next)

                    if nxt not in visited:
                        visited.add(nxt)
                        q.append((nxt, aux.a_jk[(nxt, k)]))

            aux.delta_k[k] = max(0.0, dest_arrival - b.LT_k)
            aux.t_max = max(aux.t_max, dest_arrival)

# =========================
# Upper-level: Two-Level NSGA-II (strategic)
# =========================

class TwoLevelNSGA2:
    """
    Upper-level NSGA-II on OD→route-portfolio.
    - Chromosome: PortfolioChromosome (for each (O,D) a small pool of RouteSegments)
    - Evaluation: call LowerLevelAllocator.evaluate()
    - Crossover: merge route pools by (O,D)
    - Mutation: guided by lower-level bottleneck nodes to avoid congested vertices
    """
    def __init__(self, model: MultimodalOptimizationModel,
                 batches: List[Batch],
                 pop_size: int = 30,
                 n_generations: int = 50,
                 crossover_prob: float = 0.9,
                 mutation_prob: float = 0.25,
                 max_paths_per_od: int = 3):
        self.model = model
        self.params = model.params
        self.network = model.network
        self.batches = batches
        self.pop_size = pop_size
        self.n_generations = n_generations
        self.crossover_prob = crossover_prob
        self.mutation_prob = mutation_prob
        self.max_paths_per_od = max_paths_per_od
        self.lower = LowerLevelAllocator(model)
        self.od_pairs = sorted({(b.O_k, b.DEST_k) for b in batches})

    # ---- path search (strategic, batch-agnostic) ----
    def find_paths(self, origin: str, dest: str, max_paths: int = 12, max_hops: int = 8) -> List[RouteSegment]:
        arcs = self.network['arcs']
        paths: List[RouteSegment] = []
        def dfs(node: str, nodes: List[str], modes: List[Mode]):
            if len(paths) >= max_paths:
                return
            if node == dest and len(nodes) > 1:
                paths.append(RouteSegment(nodes=list(nodes), modes=list(modes)))
                return
            if len(nodes) > max_hops:
                return
            for (i, j), arc_data in arcs.items():
                if i == node and j not in nodes:  # avoid cycles
                    ms = arc_data.get('modes', [])
                    for m in ms:
                        nodes.append(j)
                        modes.append(m)
                        dfs(j, nodes, modes)
                        nodes.pop()
                        modes.pop()
        dfs(origin, [origin], [])
        return paths

    def generate_initial_population(self) -> List[PortfolioChromosome]:
        pop = []
        for _ in range(self.pop_size):
            pool: Dict[Tuple[str, str], List[RouteSegment]] = {}
            for (o, d) in self.od_pairs:
                cand = self.find_paths(o, d, max_paths=12)
                if cand:
                    k = min(self.max_paths_per_od, len(cand))
                    pool[(o, d)] = random.sample(cand, k)
                else:
                    pool[(o, d)] = []
            pop.append(PortfolioChromosome(route_pool=pool))
        return pop

    def evaluate(self, chrom: PortfolioChromosome) -> AllocationResult:
        return self.lower.evaluate(chrom)

    # ---- genetic operators ----
    def crossover(self, p1: PortfolioChromosome, p2: PortfolioChromosome) -> PortfolioChromosome:
        child_pool: Dict[Tuple[str, str], List[RouteSegment]] = {}
        for od in self.od_pairs:
            routes1 = p1.route_pool.get(od, [])
            routes2 = p2.route_pool.get(od, [])
            # merge unique
            def sig(r: RouteSegment) -> Tuple[Tuple[str, ...], Tuple[int, ...]]:
                return (tuple(r.nodes), tuple(int(m) for m in r.modes))
            merged, seen = [], set()
            for r in (routes1 + routes2):
                s = sig(r)
                if s not in seen:
                    seen.add(s)
                    merged.append(r)
            if merged:
                k = min(self.max_paths_per_od, len(merged))
                child_pool[od] = random.sample(merged, k)
            else:
                child_pool[od] = []
        return PortfolioChromosome(route_pool=child_pool)

    def mutate(self, chrom: PortfolioChromosome, bottlenecks: List[str]) -> PortfolioChromosome:
        if not bottlenecks or random.random() > self.mutation_prob:
            return copy.deepcopy(chrom)
        mutated = copy.deepcopy(chrom)
        bad = set(bottlenecks)
        for od in self.od_pairs:
            cur = mutated.route_pool.get(od, [])
            cand_all = self.find_paths(od[0], od[1], max_paths=16)
            avoid = [r for r in cand_all if not any(n in bad for n in r.nodes)]
            new_list = []
            for r in cur:
                if any(n in bad for n in r.nodes) and avoid:
                    new_list.append(random.choice(avoid))
                else:
                    new_list.append(r)
            if not avoid and cand_all:
                for i in range(len(new_list)):
                    if random.random() < 0.3:
                        new_list[i] = random.choice(cand_all)
            if len(new_list) > self.max_paths_per_od:
                new_list = random.sample(new_list, self.max_paths_per_od)
            mutated.route_pool[od] = new_list
        return mutated

    # ---- non-dominated sorting + crowding distance ----
    @staticmethod
    def dominates(o1: Tuple[float, float, float], o2: Tuple[float, float, float]) -> bool:
        better = False
        for i in range(3):
            if o1[i] > o2[i]:
                return False
            if o1[i] < o2[i]:
                better = True
        return better

    def non_dominated_sort(self, objs: List[Tuple[float, float, float]]) -> List[List[int]]:
        n = len(objs)
        S = [[] for _ in range(n)]
        n_dom = [0] * n
        fronts = [[]]
        for p in range(n):
            for q in range(n):
                if p == q:
                    continue
                if self.dominates(objs[p], objs[q]):
                    S[p].append(q)
                elif self.dominates(objs[q], objs[p]):
                    n_dom[p] += 1
            if n_dom[p] == 0:
                fronts[0].append(p)
        i = 0
        while fronts[i]:
            nxt = []
            for p in fronts[i]:
                for q in S[p]:
                    n_dom[q] -= 1
                    if n_dom[q] == 0:
                        nxt.append(q)
            i += 1
            fronts.append(nxt)
        return [f for f in fronts if f]

    @staticmethod
    def crowding_distance(front_indices: List[int], objs: List[Tuple[float, float, float]]) -> List[float]:
        if not front_indices:
            return []
        distances = [0.0] * len(front_indices)
        nobj = 3
        local = [objs[i] for i in front_indices]
        for m in range(nobj):
            order = sorted(range(len(local)), key=lambda t: local[t][m])
            distances[order[0]] = float('inf')
            distances[order[-1]] = float('inf')
            minv = local[order[0]][m]
            maxv = local[order[-1]][m]
            if abs(maxv - minv) < 1e-12:
                continue
            for idx in range(1, len(local) - 1):
                prev_val = local[order[idx - 1]][m]
                next_val = local[order[idx + 1]][m]
                distances[order[idx]] += (next_val - prev_val) / (maxv - minv)
        return distances

    def run(self) -> List[Tuple[DecisionVariables, Tuple[float, float, float]]]:
        print(f"Starting Two-Level NSGA-II: pop={self.pop_size}, gen={self.n_generations}")
        population = self.generate_initial_population()
        last_eval: List[AllocationResult] = [None] * len(population)

        for g in range(self.n_generations):
            objs = []
            for i, chrom in enumerate(population):
                if last_eval[i] is None:
                    last_eval[i] = self.evaluate(chrom)
                objs.append(last_eval[i].objectives)

            fronts = self.non_dominated_sort(objs)
            new_population: List[PortfolioChromosome] = []
            new_last_eval: List[AllocationResult] = []

            for front in fronts:
                if len(new_population) + len(front) <= self.pop_size:
                    for idx in front:
                        new_population.append(population[idx])
                        new_last_eval.append(last_eval[idx])
                else:
                    dist = self.crowding_distance(front, objs)
                    order = sorted(range(len(front)), key=lambda t: dist[t], reverse=True)
                    for t in order:
                        if len(new_population) < self.pop_size:
                            idx = front[t]
                            new_population.append(population[idx])
                            new_last_eval.append(last_eval[idx])
                        else:
                            break
                if len(new_population) >= self.pop_size:
                    break

            # offspring
            offspring: List[PortfolioChromosome] = []
            while len(offspring) < self.pop_size:
                p1 = random.choice(new_population)
                p2 = random.choice(new_population)
                if random.random() < self.crossover_prob:
                    child = self.crossover(p1, p2)
                else:
                    child = copy.deepcopy(p1)
                parent_eval = random.choice(new_last_eval)
                child = self.mutate(child, parent_eval.bottleneck_nodes if parent_eval else [])
                offspring.append(child)

            # select next generation
            combined = new_population + offspring
            combined_eval: List[AllocationResult] = []
            combined_objs: List[Tuple[float, float, float]] = []
            for i, chrom in enumerate(combined):
                res = self.evaluate(chrom)
                combined_eval.append(res)
                combined_objs.append(res.objectives)

            fronts2 = self.non_dominated_sort(combined_objs)
            population = []
            last_eval = []
            for front in fronts2:
                if len(population) + len(front) <= self.pop_size:
                    for idx in front:
                        population.append(combined[idx])
                        last_eval.append(combined_eval[idx])
                else:
                    dist = self.crowding_distance(front, combined_objs)
                    order = sorted(range(len(front)), key=lambda t: dist[t], reverse=True)
                    for t in order:
                        if len(population) < self.pop_size:
                            idx = front[t]
                            population.append(combined[idx])
                            last_eval.append(combined_eval[idx])
                        else:
                            break
                if len(population) >= self.pop_size:
                    break

            if (g + 1) % 10 == 0:
                pf_size = len(fronts2[0]) if fronts2 else 0
                print(f"Generation {g + 1}: Pareto front size = {pf_size}")

        # final Pareto (first front)
        final_objs = [ev.objectives for ev in last_eval]
        final_fronts = self.non_dominated_sort(final_objs)
        pareto_solutions: List[Tuple[DecisionVariables, Tuple[float, float, float]]] = []
        if final_fronts:
            for idx in final_fronts[0]:
                pareto_solutions.append((last_eval[idx].dv, last_eval[idx].objectives))
        return pareto_solutions

# =========================
# Excel reader & network builder
# =========================

class ExcelDataReader:
    """Read network data from Excel"""
    def __init__(self, filepath: str):
        self.filepath = filepath
        self.df_dict = {}

    def read_all_sheets(self):
        try:
            xls = pd.ExcelFile(self.filepath)
            for sheet_name in xls.sheet_names:
                self.df_dict[sheet_name] = pd.read_excel(self.filepath, sheet_name=sheet_name)
            print(f"Loaded file: {self.filepath}")
            print(f"Sheets: {', '.join(xls.sheet_names)}")
        except Exception as e:
            print(f"Error reading Excel: {e}")
            raise

    def normalize_name(self, name) -> str:
        if pd.isna(name) or not name:
            return ""
        name = str(name).strip()
        name = re.sub(r'\s+', ' ', name)
        return name

    def load_network_parameters(self) -> NetworkParameters:
        params = NetworkParameters()
        params.V_m = {Mode.ROAD: 60.0, Mode.RAIL: 50.0, Mode.WATER: 25.0}
        params.CT_r = {'CN': 10.0, 'CA': 15.0, 'RU': 5.0, 'EE': 20.0, 'WE': 25.0}
        for mode in Mode:
            for region in ['CN', 'CA', 'RU', 'EE', 'WE']:
                if mode == Mode.ROAD:
                    params.E_mr[(mode, region)] = 0.062
                elif mode == Mode.RAIL:
                    params.E_mr[(mode, region)] = 0.025
                else:
                    params.E_mr[(mode, region)] = 0.015
        params.Theta_rm = {
            ('CN', Mode.ROAD): 1.0, ('CN', Mode.RAIL): 0.0, ('CN', Mode.WATER): 0.0,
            ('CA', Mode.ROAD): 1.0, ('CA', Mode.RAIL): 0.5, ('CA', Mode.WATER): 0.0,
            ('RU', Mode.ROAD): 0.5, ('RU', Mode.RAIL): 0.0, ('RU', Mode.WATER): 0.0,
            ('EE', Mode.ROAD): 1.0, ('EE', Mode.RAIL): 1.0, ('EE', Mode.WATER): 0.5,
            ('WE', Mode.ROAD): 1.0, ('WE', Mode.RAIL): 1.0, ('WE', Mode.WATER): 1.0,
        }
        for m1 in Mode:
            for m2 in Mode:
                if m1 != m2:
                    params.TC_jmn[(None, m1, m2)] = 50.0
                    if m1 == Mode.ROAD and m2 == Mode.RAIL:
                        params.TT_jmn[(None, m1, m2)] = 2.0
                    elif m1 == Mode.ROAD and m2 == Mode.WATER:
                        params.TT_jmn[(None, m1, m2)] = 3.0
                    elif m1 == Mode.RAIL and m2 == Mode.ROAD:
                        params.TT_jmn[(None, m1, m2)] = 2.0
                    elif m1 == Mode.RAIL and m2 == Mode.WATER:
                        params.TT_jmn[(None, m1, m2)] = 4.0
                    elif m1 == Mode.WATER and m2 == Mode.ROAD:
                        params.TT_jmn[(None, m1, m2)] = 3.0
                    elif m1 == Mode.WATER and m2 == Mode.RAIL:
                        params.TT_jmn[(None, m1, m2)] = 4.0
        # default coefficients/weights
        for node in self.get_all_nodes():
            params.W_hold[node] = 2.0
            params.W_proc[node] = 5.0
            params.W_cong[node] = 10.0
            params.BD_j[node] = 6.0
            params.CC_j[node] = 4.0
        for k in range(1, 200):
            params.P_k[k] = 20.0
        return params

    def get_all_nodes(self) -> Set[str]:
        nodes = set()
        if 'Nodes' in self.df_dict:
            df = self.df_dict['Nodes']
            for col in ['EnglishName', 'ChineseName', 'Name']:
                if col in df.columns:
                    for name in df[col]:
                        name = self.normalize_name(name)
                        if name:
                            nodes.add(name)
        for sheet_name, df in self.df_dict.items():
            if 'Arcs' in sheet_name or 'Arc' in sheet_name:
                for col in ['OriginEN', 'DestEN', 'Origin', 'Destination', 'From', 'To']:
                    if col in df.columns:
                        for name in df[col]:
                            name = self.normalize_name(name)
                            if name:
                                nodes.add(name)
        return nodes

    def load_nodes(self) -> Dict[str, dict]:
        nodes = {}
        if 'Nodes' in self.df_dict:
            df = self.df_dict['Nodes']
            for _, row in df.iterrows():
                name = None
                for col in ['EnglishName', 'Name', 'NodeName']:
                    if col in df.columns:
                        name = self.normalize_name(row[col])
                        if name:
                            break
                if not name:
                    continue
                region = 'CN'
                if 'Region' in df.columns and not pd.isna(row['Region']):
                    region = str(row['Region']).strip()
                capacity = 1e6
                for col in ['NodeCap_TEUh', 'Capacity', 'Cap']:
                    if col in df.columns and not pd.isna(row[col]):
                        capacity = float(row[col])
                        break
                nodes[name] = {
                    'region': region,
                    'capacity': capacity
                }
                border_keywords = ['Khorgos', 'Alashankou', 'Erenhot', 'Manzhouli',
                                   'Zabaikalsk', 'Brest', 'Malaszewicze', 'Małaszewicze']
                is_border = any(kw.lower() in name.lower() for kw in border_keywords)
                nodes[name]['is_border'] = is_border
        print(f"Loaded {len(nodes)} nodes")
        return nodes

    def load_arcs(self) -> Dict[Tuple[str, str], dict]:
        arcs = {}
        arc_sheets = [name for name in self.df_dict.keys() if 'Arc' in name or 'arc' in name]
        for sheet_name in arc_sheets:
            df = self.df_dict[sheet_name]
            for _, row in df.iterrows():
                origin = None
                dest = None
                for orig_col in ['OriginEN', 'Origin', 'From']:
                    if orig_col in df.columns:
                        origin = self.normalize_name(row[orig_col])
                        if origin:
                            break
                for dest_col in ['DestEN', 'Destination', 'Dest', 'To']:
                    if dest_col in df.columns:
                        dest = self.normalize_name(row[dest_col])
                        if dest:
                            break
                if not origin or not dest:
                    continue
                arc_key = (origin, dest)
                if arc_key not in arcs:
                    arcs[arc_key] = {'distances': {}, 'modes': []}
                mode_cols = {
                    'Road_km': Mode.ROAD, 'Rail_km': Mode.RAIL, 'Water_km': Mode.WATER,
                    'Road': Mode.ROAD, 'Rail': Mode.RAIL, 'Water': Mode.WATER, 'Waterway': Mode.WATER
                }
                for col, mode in mode_cols.items():
                    if col in df.columns and not pd.isna(row[col]):
                        distance = float(row[col])
                        if distance > 0:
                            arcs[arc_key]['distances'][mode] = distance
                            if mode not in arcs[arc_key]['modes']:
                                arcs[arc_key]['modes'].append(mode)
        print(f"Loaded {len(arcs)} arcs")
        return arcs

    def load_schedules(self) -> Dict[Tuple[str, str, Mode], float]:
        schedules = {}
        if 'Timetable' in self.df_dict:
            df = self.df_dict['Timetable']
            for _, row in df.iterrows():
                origin = self.normalize_name(row.get('OriginEN', ''))
                dest = self.normalize_name(row.get('DestEN', ''))
                if not origin or not dest:
                    continue
                freq_str = row.get('Frequency_per_week', None)
                if pd.isna(freq_str):
                    continue
                freq = self.parse_frequency(str(freq_str))
                if freq and freq > 0:
                    headway = 168.0 / freq
                    mode_str = str(row.get('Mode', '')).strip().upper() if 'Mode' in df.columns else ''
                    if 'RAIL' in mode_str or mode_str == '2':
                        schedules[(origin, dest, Mode.RAIL)] = headway
                    elif 'WATER' in mode_str or mode_str == '3':
                        schedules[(origin, dest, Mode.WATER)] = headway
                    else:
                        schedules[(origin, dest, Mode.RAIL)] = headway
                        schedules[(origin, dest, Mode.WATER)] = headway
        print(f"Loaded {len(schedules)} timetable entries")
        return schedules

    def parse_frequency(self, freq_str: str) -> Optional[float]:
        freq_str = freq_str.lower()
        if 'daily' in freq_str or '每天' in freq_str:
            return 7.0
        elif 'weekly' in freq_str or '每周' in freq_str:
            return 1.0
        elif 'biweekly' in freq_str:
            return 0.5
        else:
            numbers = re.findall(r'\d+(?:\.\d+)?', freq_str)
            if numbers:
                return float(numbers[0])
        return None

    def build_network_data(self) -> Tuple[NetworkParameters, dict]:
        self.read_all_sheets()
        nodes = self.load_nodes()
        arcs = self.load_arcs()
        schedules = self.load_schedules()
        params = self.load_network_parameters()

        # map node attributes
        for node_name, node_data in nodes.items():
            params.rho[node_name] = node_data['region']
            params.CAP_node[node_name] = float(node_data['capacity'])  # assume TEU/h
            params.B_j[node_name] = node_data['is_border']
            if node_data['is_border']:
                params.CAP_border[node_name] = float(node_data['capacity']) * 0.5

        # map arcs and costs
        for (origin, dest), arc_data in arcs.items():
            for mode, distance in arc_data['distances'].items():
                params.L_ij[(origin, dest)] = distance
                if mode == Mode.ROAD:
                    params.C_ijm[(origin, dest, mode)] = 1.8
                elif mode == Mode.RAIL:
                    params.C_ijm[(origin, dest, mode)] = 1.2
                else:
                    params.C_ijm[(origin, dest, mode)] = 0.8
            params.arc_modes[(origin, dest)] = arc_data['modes']
            if origin in nodes and dest in nodes:
                if nodes[origin]['region'] != nodes[dest]['region']:
                    params.I_border[(origin, dest)] = True

        # expand weekly headways into within-week departure times
        for (origin, dest, mode), headway in schedules.items():
            schedule_times = []
            t = 0.0
            while t < 168.0:
                schedule_times.append(round(t, 6))
                t += headway
            key = (origin, mode)
            if key not in params.S_jm:
                params.S_jm[key] = schedule_times
            else:
                merged = sorted(set(params.S_jm[key] + schedule_times))
                params.S_jm[key] = merged

        network = {'nodes': nodes, 'arcs': arcs, 'schedules': schedules}

        # === Optional: unify all node capacities to 250 TEU/h ===
        # for j in params.CAP_node.keys():
        #     params.CAP_node[j] = 250.0

        return params, network

# ========= allocation/split report (node perspective) =========

def build_allocation_report(model: MultimodalOptimizationModel,
                            dv: DecisionVariables) -> str:
    """
    Report per batch:
    - OD pair
    - Nodes where mode-change occurs (m → n) and the split volume (TEU)
    - Node capacity CAP_node[j] (TEU/h)
    - This batch's throughput at that node (TEU)
    """
    params = model.params

    aux = AuxiliaryVariables()
    aux.y_jmnk = model.determine_transshipment_volumes(dv)

    node_throughput = defaultdict(float)
    node_throughput_by_batch = defaultdict(float)

    for (i, j, m, k), flow in dv.f_ijmk.items():
        if flow > 0:
            node_throughput[j] += flow
            node_throughput_by_batch[(j, k)] += flow

    batch_node_splits = defaultdict(lambda: defaultdict(list))
    for (j, m, n, k), y in aux.y_jmnk.items():
        if y > 1e-9:
            batch_node_splits[k][j].append((m, n, y))

    lines = []
    lines.append("\nDetailed allocation and split information")
    lines.append("-" * 80)

    for b in model.batches:
        lines.append(f"\nBatch {b.id}: {b.O_k} -> {b.DEST_k}, demand={b.Q_k:.2f} TEU")
        has_flow = any(k2 == b.id and f > 0 for (_, _, _, k2), f in dv.f_ijmk.items())
        if not has_flow:
            lines.append("  * This batch was not allocated (likely infeasible or blocked by capacity).")
            continue

        if b.id in batch_node_splits:
            for j, items in batch_node_splits[b.id].items():
                cap = params.CAP_node.get(j, float('inf'))
                batch_through = node_throughput_by_batch.get((j, b.id), 0.0)
                lines.append(f"  - Node {j}: capacity={cap:.2f} TEU/h, this batch throughput={batch_through:.2f} TEU")
                for (m, n, y) in sorted(items, key=lambda x: -x[2]):
                    lines.append(f"      · Mode change: {Mode(m).name} → {Mode(n).name}, volume={y:.2f} TEU")
        else:
            lines.append("  * No mode change events for this batch.")

    lines.append("\nNode-level aggregate throughput (all batches)")
    lines.append("-" * 80)
    for j, tot in sorted(node_throughput.items(), key=lambda x: -x[1]):
        cap = params.CAP_node.get(j, float('inf'))
        lines.append(f"  Node {j}: capacity={cap:.2f} TEU/h, aggregate throughput={tot:.2f} TEU")

    return "\n".join(lines)

# ========= route decomposition report (path perspective) =========

def _decompose_batch_paths(model: MultimodalOptimizationModel,
                           dv: DecisionVariables,
                           batch: Batch) -> List[Tuple[List[str], List[Mode], float]]:
    """
    Residual-path decomposition:
    Repeatedly extract a simple O→D path and take the bottleneck arc flow as path flow, subtract, then continue.
    Returns: [(nodes, modes, path_flow), ...]
    """
    flow = defaultdict(float)  # key=(i,j,m)
    for (i, j, m, k), v in dv.f_ijmk.items():
        if k == batch.id and v > 1e-9:
            flow[(i, j, m)] += v

    def build_adj():
        adj = defaultdict(list)
        for (i, j, m), v in flow.items():
            if v > 1e-9:
                adj[i].append((j, m, v))
        return adj

    paths = []
    while True:
        adj = build_adj()
        if batch.O_k not in adj:
            break

        best_path_nodes = None
        best_path_modes = None
        stack = [(batch.O_k, [batch.O_k], [])]
        found = False
        while stack and not found:
            node, nodes, modes = stack.pop()
            if node == batch.DEST_k and len(nodes) > 1:
                best_path_nodes = nodes
                best_path_modes = modes
                found = True
                break
            for (j, m, v) in adj.get(node, []):
                if j in nodes:
                    continue
                stack.append((j, nodes + [j], modes + [m]))

        if not found:
            break

        min_edge = float('inf')
        for a, b, m in zip(best_path_nodes[:-1], best_path_nodes[1:], best_path_modes):
            min_edge = min(min_edge, flow[(a, b, m)])
        if min_edge < 1e-9 or min_edge == float('inf'):
            break

        paths.append((best_path_nodes, best_path_modes, min_edge))
        for a, b, m in zip(best_path_nodes[:-1], best_path_nodes[1:], best_path_modes):
            flow[(a, b, m)] -= min_edge

    return paths


def build_route_report(model: MultimodalOptimizationModel, dv: DecisionVariables) -> str:
    """
    For each batch:
      1) List the decomposed full routes (node sequence + mode sequence + TEU carried per route)
      2) Indicate mode-change nodes (from y_jmnk)
      3) Show node capacity (CAP_node[j]) and that batch's throughput at the node
    """
    params = model.params
    aux = AuxiliaryVariables()
    aux.y_jmnk = model.determine_transshipment_volumes(dv)

    batch_node_in = defaultdict(float)
    for (i, j, m, k), v in dv.f_ijmk.items():
        if v > 0:
            batch_node_in[(k, j)] += v

    lines = []
    lines.append("\nFull route decomposition (per batch)")
    lines.append("-" * 80)

    for b in model.batches:
        paths = _decompose_batch_paths(model, dv, b)
        lines.append(f"\nBatch {b.id}: {b.O_k} -> {b.DEST_k}, demand={b.Q_k:.2f} TEU")
        if not paths:
            lines.append("  * No complete O→D path was reconstructed (flows fragmented or disconnected).")
            continue

        tot = sum(p[2] for p in paths)
        lines.append(f"  · Number of paths={len(paths)}, total allocated≈{tot:.2f} TEU")
        for idx, (nodes, modes, q) in enumerate(paths, 1):
            steps = []
            for a, b2, m in zip(nodes[:-1], nodes[1:], modes):
                steps.append(f"{a} —[{Mode(m).name}]→ {b2}")
            route_str = "  ->  ".join(steps)
            lines.append(f"    Path {idx}: {route_str}  | flow={q:.2f} TEU")

        splits = [(j, m, n, k, y) for (j, m, n, k), y in aux.y_jmnk.items() if k == b.id and y > 1e-9]
        if splits:
            lines.append("  · Mode-change nodes:")
            for (j, m, n, _, y) in sorted(splits, key=lambda x: -x[4]):
                cap = params.CAP_node.get(j, float('inf'))
                thr = batch_node_in.get((b.id, j), 0.0)
                lines.append(f"      {j}: {Mode(m).name}→{Mode(n).name}, split volume={y:.2f} TEU; "
                             f"capacity={cap:.2f} TEU/h, this batch throughput={thr:.2f} TEU")
        else:
            lines.append("  · No mode changes for this batch.")

    return "\n".join(lines)

# =========================
# Main
# =========================

def main():
    """Entry point: run two-level NSGA-II"""
    import sys

    possible_paths = [
        'extended.xlsx',
        'Low Carbon/extended.xlsx',
        'low carbon/extended.xlsx',
        '../Low Carbon/extended.xlsx',
        '../extended.xlsx'
    ]
    excel_file = None
    for path in possible_paths:
        if os.path.exists(path):
            excel_file = path
            break
    if not excel_file:
        print("Error: extended.xlsx not found.")
        print("Make sure the file is located at one of:")
        for path in possible_paths:
            print(f"  - {path}")
        sys.exit(1)

    print(f"Using file: {excel_file}")
    print("=" * 80)
    reader = ExcelDataReader(excel_file)
    params, network = reader.build_network_data()

    # Batches (example set)
    batches: List[Batch] = []
    major_origins = ["Xi'an", "Chongqing", "Zhengzhou", "Wuhan"]
    major_destinations = ["Berlin", "Hamburg", "Duisburg", "Munich"]
    batch_id = 1
    for origin in major_origins:
        for dest in major_destinations:
            if origin in network['nodes'] and dest in network['nodes']:
                batch = Batch(
                    id=batch_id,
                    O_k=origin,
                    DEST_k=dest,
                    Q_k=random.uniform(50, 150),
                    ET_k=0.0,
                    LT_k=240.0
                )
                batches.append(batch)
                batch_id += 1

    # Fallback: small CN→EU grid if none created
    if not batches:
        all_nodes = list(network['nodes'].keys())
        cn_nodes = [n for n in all_nodes if network['nodes'][n]['region'] == 'CN']
        eu_nodes = [n for n in all_nodes if network['nodes'][n]['region'] in ['WE', 'EE']]
        if cn_nodes and eu_nodes:
            for i in range(min(3, len(cn_nodes))):
                for j in range(min(3, len(eu_nodes))):
                    batch = Batch(
                        id=i * 3 + j + 1,
                        O_k=cn_nodes[i],
                        DEST_k=eu_nodes[j],
                        Q_k=100.0,
                        ET_k=0.0,
                        LT_k=240.0
                    )
                    batches.append(batch)

    print(f"Created {len(batches)} batches:")
    for batch in batches[:12]:
        print(f"  Batch {batch.id}: {batch.O_k} -> {batch.DEST_k}, {batch.Q_k:.1f} TEU")
    if len(batches) > 12:
        print(f"  ... (remaining {len(batches) - 12} batches omitted)")

    print("\nStarting two-level optimisation...")
    print("=" * 80)
    model = MultimodalOptimizationModel(params, network, batches)

    # Two-level NSGA-II
    tl_nsga2 = TwoLevelNSGA2(
        model=model,
        batches=batches,
        pop_size=30,
        n_generations=50,
        crossover_prob=0.9,
        mutation_prob=0.25,
        max_paths_per_od=3
    )
    pareto_solutions = tl_nsga2.run()

    print("\n" + "=" * 80)
    print("PARETO-optimal solutions")
    print("=" * 80)
    if pareto_solutions:
        pareto_solutions.sort(key=lambda x: x[1][0])   # sort by cost
        for i, (solution, objectives) in enumerate(pareto_solutions[:10], 1):
            print(f"\nSolution {i}:")
            print(f"  Obj1 - Total cost: ${objectives[0]:,.2f}")
            print(f"  Obj2 - Total emissions: {objectives[1]:.2f} tCO2")
            print(f"  Obj3 - Makespan: {objectives[2]:.2f} h ({objectives[2]/24:.1f} days)")
    else:
        print("No feasible solution found; please verify network connectivity/data quality.")

    # Output file
    output_file = 'optimization_results.txt'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("Multi-batch Multimodal Transport Results (Two-Level NSGA-II)\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"Data source: {excel_file}\n")
        f.write(f"Batches: {len(batches)}\n")
        f.write(f"Nodes: {len(network['nodes'])}\n")
        f.write(f"Arcs: {len(network['arcs'])}\n\n")
        if pareto_solutions:
            f.write("Pareto front:\n")
            for i, (_, objectives) in enumerate(pareto_solutions, 1):
                f.write(f"Sol {i}: Cost=${objectives[0]:.2f}, Emissions={objectives[1]:.2f} tCO2, Time={objectives[2]:.2f} h\n")

            # choose the minimum-cost solution for detailed reporting
            best_sol_dv, _ = sorted(pareto_solutions, key=lambda x: x[1][0])[0]
            detail_report = build_allocation_report(model, best_sol_dv)
            f.write("\n")
            f.write(detail_report)

            route_report = build_route_report(model, best_sol_dv)
            f.write("\n")
            f.write(route_report)

    print(f"\nResults written to: {output_file}")

if __name__ == "__main__":
    main()
