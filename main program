#!/usr/bin/env python
# -*- coding: utf-8 -*-

import math
import random
from dataclasses import dataclass, field
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401


# 全局设置
TIME_BUCKET_H = 1.0
CHINA_REGIONS = {"CN", "China"}
EUROPE_REGIONS = {"WE", "EE", "EU", "Europe"}


# --------------------
# 数据结构
# --------------------

@dataclass
class Arc:
    from_node: str
    to_node: str
    mode: str
    distance: float
    capacity: float       # TEU / time bucket
    cost_per_teu_km: float
    emission_per_teu_km: float
    speed_kmh: float


@dataclass
class TimetableEntry:
    from_node: str
    to_node: str
    mode: str
    frequency_per_week: float
    first_departure_hour: float
    headway_hours: float


@dataclass
class Batch:
    batch_id: int
    origin: str
    destination: str
    quantity: float
    ET: float
    LT: float


@dataclass
class Path:
    path_id: int
    origin: str
    destination: str
    nodes: List[str]
    modes: List[str]
    arcs: List[Arc]
    base_cost_per_teu: float
    base_emission_per_teu: float
    base_travel_time_h: float

    # 比较结构是否相同
    def __eq__(self, other):
        if not isinstance(other, Path):
            return NotImplemented
        return self.nodes == other.nodes and self.modes == other.modes

    def __hash__(self):
        return hash((tuple(self.nodes), tuple(self.modes)))


@dataclass
class PathAllocation:
    path: Path
    share: float

    def __repr__(self):
        chain = ""
        for i, node in enumerate(self.path.nodes[:-1]):
            mode = self.path.modes[i]
            chain += f"{node}--({mode})-->"
        chain += self.path.nodes[-1]
        return f"\n    {{ Structure: [{chain}], Share: {self.share:.2%} }}"


@dataclass(eq=False)
class Individual:
    # key: (origin, dest) -> List[PathAllocation]
    od_allocations: Dict[Tuple[str, str], List[PathAllocation]] = field(default_factory=dict)
    # (cost, emission, time)
    objectives: Tuple[float, float, float] = (float("inf"), float("inf"), float("inf"))


# --------------------
# 辅助：合并/归一化 share
# --------------------

def clone_gene(alloc: PathAllocation) -> PathAllocation:
    return PathAllocation(path=alloc.path, share=alloc.share)


def merge_and_normalize(allocs: List[PathAllocation]) -> List[PathAllocation]:
    if not allocs:
        return []

    merged_map: Dict[Path, float] = {}
    for a in allocs:
        merged_map[a.path] = merged_map.get(a.path, 0.0) + a.share

    unique_allocs = [PathAllocation(path=p, share=s) for p, s in merged_map.items()]

    total_share = sum(a.share for a in unique_allocs)
    if total_share <= 1e-9:
        if not unique_allocs:
            return []
        avg = 1.0 / len(unique_allocs)
        for a in unique_allocs:
            a.share = avg
    else:
        factor = 1.0 / total_share
        for a in unique_allocs:
            a.share *= factor

    # 去掉很小的份额
    unique_allocs = [a for a in unique_allocs if a.share > 0.001]

    # 再校一次总和
    if unique_allocs:
        final_total = sum(a.share for a in unique_allocs)
        if abs(final_total - 1.0) > 1e-6:
            for a in unique_allocs:
                a.share /= final_total

    return unique_allocs


# --------------------
# 读取数据 & 网络构建
# --------------------

def load_network_from_extended(filename: str):
    try:
        xls = pd.ExcelFile(filename)
    except FileNotFoundError:
        print(f"Error: File '{filename}' not found.")
        exit(1)

    # Nodes
    nodes_df = pd.read_excel(xls, "Nodes")
    node_names = nodes_df["EnglishName"].astype(str).tolist()
    node_region = dict(zip(nodes_df["EnglishName"].astype(str),
                           nodes_df["Region"].astype(str)))

    # Arcs
    arcs_df = pd.read_excel(xls, "Arcs_All")
    arcs: List[Arc] = []

    
    DAILY_HOURS = 24.0

    for _, row in arcs_df.iterrows():
        mode_raw = str(row["Mode"]).strip().lower()
        if mode_raw == "road":
            speed = 75.0
        elif mode_raw == "water":
            speed = 30.0
        else:
            speed = 50.0
        mode = "rail" if mode_raw == "rail" else mode_raw

        dist_str = str(row["Distance_km"])
        cleaned = "".join(ch for ch in dist_str if (ch.isdigit() or ch == "."))
        distance = float(cleaned) if cleaned else 0.0

        if "Capacity_TEU" in arcs_df.columns and not pd.isna(row["Capacity_TEU"]):
            raw_cap = float(row["Capacity_TEU"])
        else:
            raw_cap = 1e9
        capacity = raw_cap * (TIME_BUCKET_H / DAILY_HOURS)

        arcs.append(Arc(
            from_node=str(row["OriginEN"]).strip(),
            to_node=str(row["DestEN"]).strip(),
            mode=mode,
            distance=distance,
            capacity=capacity,
            cost_per_teu_km=float(row["Cost_$_per_km"]),
            emission_per_teu_km=float(row["Emission_gCO2_per_tkm"]),
            speed_kmh=speed
        ))

    # Timetable
    tdf = pd.read_excel(xls, "Timetable")
    timetables: List[TimetableEntry] = []
    for _, row in tdf.iterrows():
        freq = float(row["Frequency_per_week"])
        hd = row["Headway_Hours"]
        hd = 168.0 / max(freq, 1.0) if pd.isna(hd) else float(hd)

        v = row["FirstDepartureHour"]
        fd = 0.0
        if not pd.isna(v):
            try:
                s = str(v)
                fd = float(s.split(":")[0]) if ":" in s else float(s)
            except Exception:
                fd = 0.0

        timetables.append(TimetableEntry(
            from_node=str(row["OriginEN"]).strip(),
            to_node=str(row["DestEN"]).strip(),
            mode=str(row["Mode"]).strip().lower(),
            frequency_per_week=freq,
            first_departure_hour=fd,
            headway_hours=hd
        ))

    # Batches
    bdf = pd.read_excel(xls, "Batches")
    batches: List[Batch] = []
    for _, row in bdf.iterrows():
        origin = str(row["OriginEN"]).strip()
        dest = str(row["DestEN"]).strip()
        o_reg = node_region.get(origin)
        d_reg = node_region.get(dest)

        if o_reg in CHINA_REGIONS and d_reg in EUROPE_REGIONS:
            batches.append(Batch(
                batch_id=int(row["BatchID"]),
                origin=origin,
                destination=dest,
                quantity=float(row["QuantityTEU"]),
                ET=float(row["ET"]),
                LT=float(row["LT"])
            ))

    return node_names, arcs, timetables, batches


def build_graph(arcs: List[Arc]) -> Dict[str, List[Tuple[str, Arc]]]:
    g: Dict[str, List[Tuple[str, Arc]]] = {}
    for a in arcs:
        g.setdefault(a.from_node, []).append((a.to_node, a))
    return g


def build_timetable_dict(timetables: List[TimetableEntry]) -> Dict[Tuple[str, str, str], List[TimetableEntry]]:
    tt_dict: Dict[Tuple[str, str, str], List[TimetableEntry]] = {}
    for t in timetables:
        key = (t.from_node, t.to_node, t.mode)
        tt_dict.setdefault(key, []).append(t)
    return tt_dict


def random_dfs_paths(graph, origin, dest, max_len=8, max_paths=50) -> List[List[Arc]]:
    paths: List[List[Arc]] = []

    def dfs(node, cur_arcs, visited):
        if len(paths) >= max_paths or len(cur_arcs) > max_len:
            return
        if node == dest and cur_arcs:
            paths.append(cur_arcs.copy())
            return

        neighbors = graph.get(node, [])
        random.shuffle(neighbors)

        for nxt, arc in neighbors:
            if nxt in visited:
                continue
            dfs(nxt, cur_arcs + [arc], visited | {nxt})

    dfs(origin, [], {origin})
    return paths


def build_path_library(node_names, arcs, batches, timetables) -> Dict[Tuple[str, str], List[Path]]:
    graph = build_graph(arcs)
    path_lib: Dict[Tuple[str, str], List[Path]] = {}
    next_path_id = 0

    for batch in batches:
        od = (batch.origin, batch.destination)
        if od in path_lib:
            continue

        arc_paths = random_dfs_paths(graph, batch.origin, batch.destination)
        paths_for_od: List[Path] = []

        for arc_seq in arc_paths:
            nodes = [arc_seq[0].from_node] + [a.to_node for a in arc_seq]
            modes = [a.mode for a in arc_seq]

            paths_for_od.append(Path(
                path_id=next_path_id,
                origin=batch.origin,
                destination=batch.destination,
                nodes=nodes,
                modes=modes,
                arcs=arc_seq,
                base_cost_per_teu=sum(a.cost_per_teu_km * a.distance for a in arc_seq),
                base_emission_per_teu=sum(a.emission_per_teu_km * a.distance for a in arc_seq),
                base_travel_time_h=sum(a.distance / max(a.speed_kmh, 1.0) for a in arc_seq),
            ))
            next_path_id += 1

        if paths_for_od:
            paths_for_od.sort(key=lambda p: p.base_cost_per_teu)
            path_lib[od] = paths_for_od[:20]

    return path_lib


# --------------------
# 评估：时间 + 容量
# --------------------

def simulate_path_time_capacity(path: Path, batch: Batch, flow: float, tt_dict,
                                arc_flow_map) -> float:
    """
    从 batch.ET 出发，沿 path 计算到达时间，同时按 (arc, time bucket) 统计流量。
    road 不看时刻表，rail/water 按 headway 等下一班。
    """
    t = batch.ET
    for arc in path.arcs:
        travel_time = arc.distance / max(arc.speed_kmh, 1.0)

        if arc.mode == "road":
            entries = []
        else:
            key = (arc.from_node, arc.to_node, arc.mode)
            entries = tt_dict.get(key, [])

        if not entries:
            dep = t
        else:
            e = entries[0]
            if t <= e.first_departure_hour:
                dep = e.first_departure_hour
            else:
                waited = (t - e.first_departure_hour)
                n = math.ceil(waited / e.headway_hours)
                dep = e.first_departure_hour + n * e.headway_hours

        arr = dep + travel_time

        start_slot = int(dep)
        key = (arc.from_node, arc.to_node, arc.mode)
        slot_key = (key, start_slot)
        arc_flow_map[slot_key] = arc_flow_map.get(slot_key, 0.0) + flow

        t = arr

    return t - batch.ET


def evaluate_individual(ind: Individual, batches, path_lib, arcs, tt_dict):
    total_cost = 0.0
    total_emission = 0.0
    makespan = 0.0

    arc_flow_map: Dict[Tuple[Tuple[str, str, str], int], float] = {}
    arc_caps = {(a.from_node, a.to_node, a.mode): a.capacity for a in arcs}

    penalty = 0.0

    for batch in batches:
        od = (batch.origin, batch.destination)
        allocs = ind.od_allocations.get(od, [])

        if not allocs:
            penalty += 1e9
            continue

        batch_finish_time = 0.0

        for alloc in allocs:
            if alloc.share <= 1e-6:
                continue

            flow = alloc.share * batch.quantity
            path = alloc.path

            total_cost += path.base_cost_per_teu * flow
            total_emission += path.base_emission_per_teu * flow

            travel_time = simulate_path_time_capacity(path, batch, flow, tt_dict, arc_flow_map)
            arrival_time = batch.ET + travel_time

            batch_finish_time = max(batch_finish_time, arrival_time)

            if arrival_time > batch.LT:
                penalty += (arrival_time - batch.LT) * 1000.0

        makespan = max(makespan, batch_finish_time)

    for (key, slot), flow in arc_flow_map.items():
        cap = arc_caps.get(key, 1e9)
        if flow > cap:
            penalty += (flow - cap) * 5000.0

    ind.objectives = (total_cost + penalty,
                      total_emission + penalty,
                      makespan)


# --------------------
# 遗传算子
# --------------------

def crossover_structural(ind1: Individual, ind2: Individual,
                         batches: List[Batch]) -> Tuple[Individual, Individual]:
    child1 = Individual()
    child2 = Individual()

    for batch in batches:
        od = (batch.origin, batch.destination)
        genes1 = ind1.od_allocations.get(od, [])
        genes2 = ind2.od_allocations.get(od, [])

        if not genes1 and not genes2:
            continue
        if not genes1:
            child1.od_allocations[od] = [clone_gene(g) for g in genes2]
            child2.od_allocations[od] = [clone_gene(g) for g in genes2]
            continue
        if not genes2:
            child1.od_allocations[od] = [clone_gene(g) for g in genes1]
            child2.od_allocations[od] = [clone_gene(g) for g in genes1]
            continue

        cut1 = random.randint(0, len(genes1))
        cut2 = random.randint(0, len(genes2))

        c1_genes = [clone_gene(g) for g in genes1[:cut1]] + \
                   [clone_gene(g) for g in genes2[cut2:]]
        c2_genes = [clone_gene(g) for g in genes2[:cut2]] + \
                   [clone_gene(g) for g in genes1[cut1:]]

        child1.od_allocations[od] = merge_and_normalize(c1_genes)
        child2.od_allocations[od] = merge_and_normalize(c2_genes)

    return child1, child2


def mutate_structural(ind: Individual, batches: List[Batch],
                      path_lib: Dict[Tuple[str, str], List[Path]],
                      p_add=0.3, p_del=0.3, p_mod=0.3):
    batch = random.choice(batches)
    od = (batch.origin, batch.destination)
    allocs = ind.od_allocations.get(od, [])
    paths_in_lib = path_lib.get(od, [])

    if not paths_in_lib:
        return

    if random.random() < p_add:
        current_structures = {a.path for a in allocs}
        candidates = [p for p in paths_in_lib if p not in current_structures]
        if candidates:
            new_path = random.choice(candidates)
            allocs.append(PathAllocation(path=new_path, share=0.2))

    if random.random() < p_del:
        if len(allocs) > 1:
            allocs.pop(random.randint(0, len(allocs) - 1))

    if random.random() < p_mod and allocs:
        target = random.choice(allocs)
        target.share *= random.uniform(0.5, 1.5)

    ind.od_allocations[od] = merge_and_normalize(allocs)


# --------------------
# Hypervolume
# --------------------

class HypervolumeCalculator:
    def __init__(self, ref_point: Tuple[float, float, float], num_samples=10000):
        self.ref_point = np.array(ref_point, dtype=float)
        self.num_samples = num_samples
        self.ideal_point = np.zeros(3, dtype=float)
        self.samples = np.random.uniform(low=self.ideal_point,
                                         high=self.ref_point,
                                         size=(self.num_samples, 3))
        self.total_volume = float(np.prod(self.ref_point))

    def calculate(self, pareto_front_inds: List[Individual]) -> float:
        if not pareto_front_inds:
            return 0.0

        front_objs = np.array([ind.objectives for ind in pareto_front_inds], dtype=float)
        valid_mask = np.all(front_objs <= self.ref_point, axis=1)
        valid_objs = front_objs[valid_mask]
        if len(valid_objs) == 0:
            return 0.0

        S = self.samples[:, np.newaxis, :]
        O = valid_objs[np.newaxis, :, :]

        is_dominated = np.all(O <= S, axis=2)
        dominated_samples = np.any(is_dominated, axis=1)

        ratio = np.sum(dominated_samples) / float(self.num_samples)
        return ratio * self.total_volume


# --------------------
# NSGA-II 主流程
# --------------------

def random_initial_individual(batches, path_lib, max_paths=3) -> Individual:
    ind = Individual()
    for batch in batches:
        od = (batch.origin, batch.destination)
        paths = path_lib.get(od, [])
        if not paths:
            continue

        k = random.randint(1, min(max_paths, len(paths)))
        chosen = random.sample(paths, k)
        raw_allocs = [PathAllocation(path=p, share=random.random())
                      for p in chosen]
        ind.od_allocations[od] = merge_and_normalize(raw_allocs)
    return ind


def dominates(a: Individual, b: Individual) -> bool:
    return all(x <= y for x, y in zip(a.objectives, b.objectives)) and \
           any(x < y for x, y in zip(a.objectives, b.objectives))


def non_dominated_sort(pop: List[Individual]) -> List[List[Individual]]:
    S: Dict[Individual, List[Individual]] = {p: [] for p in pop}
    n: Dict[Individual, int] = {p: 0 for p in pop}
    fronts: List[List[Individual]] = [[]]

    for p in pop:
        for q in pop:
            if p is q:
                continue
            if dominates(p, q):
                S[p].append(q)
            elif dominates(q, p):
                n[p] += 1
        if n[p] == 0:
            fronts[0].append(p)

    i = 0
    while i < len(fronts) and fronts[i]:
        next_front: List[Individual] = []
        for p in fronts[i]:
            for q in S[p]:
                n[q] -= 1
                if n[q] == 0:
                    next_front.append(q)
        i += 1
        fronts.append(next_front)

    return fronts[:-1]


def crowding_distance(front: List[Individual]) -> Dict[Individual, float]:
    l = len(front)
    d: Dict[Individual, float] = {ind: 0.0 for ind in front}
    if l == 0:
        return d

    m = len(front[0].objectives)
    for i in range(m):
        front.sort(key=lambda x: x.objectives[i])
        d[front[0]] = float('inf')
        d[front[-1]] = float('inf')
        rng = front[-1].objectives[i] - front[0].objectives[i]
        if rng == 0:
            continue
        for j in range(1, l - 1):
            d[front[j]] += (front[j + 1].objectives[i] -
                            front[j - 1].objectives[i]) / rng
    return d


def tournament_select(pop: List[Individual],
                      dists: Dict[Individual, float],
                      ranks: Dict[Individual, int]) -> Individual:
    a, b = random.sample(pop, 2)
    if ranks[a] < ranks[b]:
        return a
    if ranks[b] < ranks[a]:
        return b
    if dists[a] > dists[b]:
        return a
    return b


def run_nsga2_analytics(filename="data.xlsx",
                        pop_size=50,
                        generations=30):
    print("Loading data...")
    node_names, arcs, timetables, batches = load_network_from_extended(filename)

    print("Pre-processing timetable...")
    tt_dict = build_timetable_dict(timetables)

    print("Building path library...")
    path_lib = build_path_library(node_names, arcs, batches, timetables)

    population: List[Individual] = []
    for _ in range(pop_size):
        ind = random_initial_individual(batches, path_lib)
        evaluate_individual(ind, batches, path_lib, arcs, tt_dict)
        population.append(ind)

    all_objs = np.array([ind.objectives for ind in population], dtype=float)
    max_vals = np.max(all_objs, axis=0)
    ref_point = max_vals * 1.2
    hv_calc = HypervolumeCalculator(ref_point=tuple(ref_point),
                                    num_samples=10000)
    print(f"Reference point for HV: {ref_point}")

    history_fronts: List[Tuple[int, List[Tuple[float, float, float]]]] = []
    hv_history: List[float] = []

    for gen in range(generations):
        fronts = non_dominated_sort(population)

        current_front_objs = [ind.objectives for ind in fronts[0]]
        history_fronts.append((gen, current_front_objs))

        current_hv = hv_calc.calculate(fronts[0])
        hv_history.append(current_hv)

        best_f1 = min(obj[0] for obj in current_front_objs)
        print(f"Gen {gen}: best cost={best_f1:.0f}, "
              f"front size={len(fronts[0])}, HV={current_hv:.2e}")

        ranks: Dict[Individual, int] = {}
        for r, front in enumerate(fronts):
            for ind in front:
                ranks[ind] = r

        dists: Dict[Individual, float] = {}
        for front in fronts:
            dists.update(crowding_distance(front))

        mating_pool: List[Individual] = []
        while len(mating_pool) < pop_size:
            mating_pool.append(tournament_select(population, dists, ranks))

        offspring: List[Individual] = []
        while len(offspring) < pop_size:
            if random.random() < 0.9:
                p1, p2 = random.sample(mating_pool, 2)
                c1, c2 = crossover_structural(p1, p2, batches)
            else:
                c1 = random_initial_individual(batches, path_lib)
                c2 = random_initial_individual(batches, path_lib)

            if random.random() < 0.3:
                mutate_structural(c1, batches, path_lib)
            if random.random() < 0.3:
                mutate_structural(c2, batches, path_lib)

            evaluate_individual(c1, batches, path_lib, arcs, tt_dict)
            evaluate_individual(c2, batches, path_lib, arcs, tt_dict)
            offspring.append(c1)
            offspring.append(c2)

        combined = population + offspring
        fronts = non_dominated_sort(combined)
        new_pop: List[Individual] = []
        for front in fronts:
            if len(new_pop) + len(front) <= pop_size:
                new_pop.extend(front)
            else:
                d = crowding_distance(front)
                front.sort(key=lambda x: d[x], reverse=True)
                new_pop.extend(front[:pop_size - len(new_pop)])
                break
        population = new_pop

    final_fronts = non_dominated_sort(population)
    return population, final_fronts[0], batches, history_fronts, hv_history


# --------------------
# 输出 & 绘图
# --------------------

def print_pure_structure(ind: Individual, batches: List[Batch], sol_name="Solution"):
    print(f"\n===== {sol_name} (Node+Mode | Share) =====")
    for batch in batches:
        od = (batch.origin, batch.destination)
        allocs = ind.od_allocations.get(od, [])
        if not allocs:
            continue

        print(f"\nOD: {batch.origin} -> {batch.destination}\n")
        for a in allocs:
            print(a)


def save_pareto_solutions(pareto: List[Individual],
                          batches: List[Batch],
                          filename: str = "result.txt"):
    with open(filename, "w", encoding="utf-8") as f:
        f.write("NSGA-II Pareto solutions\n\n")
        for i, ind in enumerate(pareto):
            cost, emit, time_ = ind.objectives
            f.write(f"===== Pareto Sol {i} =====\n")
            f.write(f"Objectives: Cost={cost:.2f}, "
                    f"Emission={emit:.2f}, Time={time_:.2f}\n\n")

            for batch in batches:
                od = (batch.origin, batch.destination)
                allocs = ind.od_allocations.get(od, [])
                if not allocs:
                    continue

                f.write(f"OD: {batch.origin} -> {batch.destination}\n\n")
                for a in allocs:
                    f.write(str(a) + "\n")
                f.write("\n")

            f.write("\n")

    print(f"Saved {len(pareto)} Pareto solutions to {filename}")


def plot_results(history_fronts, hv_history):
    fig = plt.figure(figsize=(18, 8))

    # 3D Pareto front
    ax1 = fig.add_subplot(121, projection='3d')
    num_gens = len(history_fronts)
    colors = cm.viridis(np.linspace(0, 1, num_gens))

    for gen, objs in history_fronts:
        if not objs:
            continue
        xs = [o[0] for o in objs]
        ys = [o[1] for o in objs]
        zs = [o[2] for o in objs]

        base = max(1, num_gens - 1)
        alpha = 0.2 + 0.8 * (gen / base)
        s = 20 + 30 * (gen / base)

        label = f"Gen {gen}" if gen in [0, num_gens - 1] else ""
        ax1.scatter(xs, ys, zs,
                    color=colors[gen],
                    alpha=alpha,
                    s=s,
                    label=label)

    ax1.set_xlabel('Cost')
    ax1.set_ylabel('Emission')
    ax1.set_zlabel('Time')
    ax1.set_title('Pareto front (by generation)')
    ax1.legend()

    # HV 曲线
    ax2 = fig.add_subplot(122)
    ax2.plot(range(len(hv_history)),
             hv_history,
             marker='o',
             linestyle='-',
             markersize=4)
    ax2.set_xlabel('Generation')
    ax2.set_ylabel('Hypervolume (approx)')
    ax2.set_title('HV convergence')
    ax2.grid(True)

    plt.tight_layout()

    # 不同视角的截图
    ax1.view_init(elev=30, azim=45)
    plt.savefig('nsga2_view_1_std.png', dpi=300)

    ax1.view_init(elev=20, azim=135)
    plt.savefig('nsga2_view_2_side.png', dpi=300)

    ax1.view_init(elev=60, azim=45)
    plt.savefig('nsga2_view_3_top.png', dpi=300)

    ax1.view_init(elev=25, azim=210)
    plt.savefig('nsga2_view_4_other.png', dpi=300)

    plt.show()


if __name__ == "__main__":
    pop, pareto, batches, h_fronts, h_hv = run_nsga2_analytics(
        filename="data.xlsx",
        pop_size=60,
        generations=50
    )

    for i, ind in enumerate(pareto[:3]):
        print_pure_structure(ind, batches, f"Pareto Sol {i}")

    save_pareto_solutions(pareto, batches, filename="result.txt")
    plot_results(h_fronts, h_hv)
