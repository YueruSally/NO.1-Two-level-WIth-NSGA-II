import math
import random
from dataclasses import dataclass, field
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd

# 时间分桶大小（小时）
TIME_BUCKET_H = 1.0


# 数据结构定义


@dataclass
class Arc:
    from_node: str
    to_node: str
    mode: str
    distance: float
    capacity: float              # 每个时间段容量（TEU）
    cost_per_teu_km: float
    emission_per_teu_km: float
    speed_kmh: float


@dataclass
class TimetableEntry:
    from_node: str
    to_node: str
    mode: str
    frequency_per_week: float
    first_departure_hour: float
    headway_hours: float


@dataclass
class Batch:
    batch_id: int
    origin: str
    destination: str
    quantity: float
    ET: float
    LT: float


@dataclass
class Path:
    path_id: int
    origin: str
    destination: str
    nodes: List[str]
    modes: List[str]
    arcs: List[Arc]
    base_cost_per_teu: float
    base_emission_per_teu: float
    base_travel_time_h: float


@dataclass
class PathAllocation:
    path_id: int
    share: float


@dataclass(eq=False)
class Individual:
    # 变长度编码：每个 OD 对应一个可变长度路径集合
    od_allocations: Dict[Tuple[str, str], List[PathAllocation]] = field(default_factory=dict)
    objectives: Tuple[float, float, float] = (float("inf"), float("inf"), float("inf"))


# Excel 数据读取


def load_network_from_extended(filename: str):
    """
    从 extended.xlsx 中读取 Nodes, Arcs_All, Timetable, Batches。
    """
    xls = pd.ExcelFile(filename)

    # Nodes
    if "Nodes" not in xls.sheet_names:
        raise ValueError("extended.xlsx 中必须包含 Nodes 表")
    nodes_df = pd.read_excel(xls, "Nodes")
    for c in ["EnglishName", "Region"]:
        if c not in nodes_df.columns:
            raise ValueError(f"Nodes 表中缺少列: {c}")

    node_names = nodes_df["EnglishName"].astype(str).tolist()
    node_region = dict(
        zip(
            nodes_df["EnglishName"].astype(str),
            nodes_df["Region"].astype(str),
        )
    )

    # Arcs_All
    if "Arcs_All" not in xls.sheet_names:
        raise ValueError("extended.xlsx 中必须包含 Arcs_All 表")
    arcs_df = pd.read_excel(xls, "Arcs_All")
    required_arc_cols = [
        "OriginEN", "DestEN", "Mode",
        "Distance_km", "Cost_$_per_km", "Emission_gCO2_per_tkm"
    ]
    for c in required_arc_cols:
        if c not in arcs_df.columns:
            raise ValueError(f"Arcs_All 表中缺少列: {c}")

    has_cap_col = "Capacity_TEU" in arcs_df.columns

    arcs: List[Arc] = []
    for _, row in arcs_df.iterrows():
        mode_raw = str(row["Mode"]).strip().lower()
        if mode_raw == "rail":
            mode = "rail"
            speed = 50.0
        elif mode_raw == "road":
            mode = "road"
            speed = 75.0
        elif mode_raw == "water":
            mode = "water"
            speed = 30.0
        else:
            mode = mode_raw
            speed = 50.0

        dist_str = str(row["Distance_km"])
        cleaned = "".join(ch for ch in dist_str if (ch.isdigit() or ch == "."))
        distance = float(cleaned) if cleaned else 0.0

        cost = float(row["Cost_$_per_km"])
        emis = float(row["Emission_gCO2_per_tkm"])

        if has_cap_col and not pd.isna(row["Capacity_TEU"]):
            capacity = float(row["Capacity_TEU"])
        else:
            capacity = 1e9

        arcs.append(
            Arc(
                from_node=str(row["OriginEN"]).strip(),
                to_node=str(row["DestEN"]).strip(),
                mode=mode,
                distance=distance,
                capacity=capacity,
                cost_per_teu_km=cost,
                emission_per_teu_km=emis,
                speed_kmh=speed,
            )
        )

    # Timetable
    if "Timetable" not in xls.sheet_names:
        raise ValueError("extended.xlsx 中必须包含 Timetable 表")
    tdf = pd.read_excel(xls, "Timetable")
    required_tt_cols = [
        "OriginEN", "DestEN", "Mode",
        "Frequency_per_week", "FirstDepartureHour", "Headway_Hours"
    ]
    for c in required_tt_cols:
        if c not in tdf.columns:
            raise ValueError(f"Timetable 表中缺少列: {c}")

    timetables: List[TimetableEntry] = []
    for _, row in tdf.iterrows():
        mode_raw = str(row["Mode"]).strip().lower()
        if mode_raw == "rail":
            mode = "rail"
        elif mode_raw == "road":
            mode = "road"
        elif mode_raw == "water":
            mode = "water"
        else:
            mode = mode_raw

        freq = float(row["Frequency_per_week"])

        v = row["FirstDepartureHour"]
        if pd.isna(v):
            fd = 0.0
        else:
            s = str(v)
            if ":" in s:
                try:
                    fd = float(s.split(":")[0])
                except Exception:
                    fd = 0.0
            else:
                fd = float(s)

        hd = row["Headway_Hours"]
        if pd.isna(hd):
            hd = 168.0 / max(freq, 1.0)
        else:
            hd = float(hd)

        timetables.append(
            TimetableEntry(
                from_node=str(row["OriginEN"]).strip(),
                to_node=str(row["DestEN"]).strip(),
                mode=mode,
                frequency_per_week=freq,
                first_departure_hour=fd,
                headway_hours=hd,
            )
        )

    # Batches
    if "Batches" not in xls.sheet_names:
        raise ValueError(
            "extended.xlsx 中必须包含 Batches 表，并包含列: "
            "BatchID, OriginEN, DestEN, QuantityTEU, ET, LT"
        )
    bdf = pd.read_excel(xls, "Batches")
    required_b_cols = ["BatchID", "OriginEN", "DestEN", "QuantityTEU", "ET", "LT"]
    for c in required_b_cols:
        if c not in bdf.columns:
            raise ValueError(f"Batches 表中缺少列: {c}")

    batches: List[Batch] = []
    for _, row in bdf.iterrows():
        bid = int(row["BatchID"])
        origin = str(row["OriginEN"]).strip()
        dest = str(row["DestEN"]).strip()
        qty = float(row["QuantityTEU"])
        et = float(row["ET"])
        lt = float(row["LT"])
        batches.append(
            Batch(
                batch_id=bid,
                origin=origin,
                destination=dest,
                quantity=qty,
                ET=et,
                LT=lt,
            )
        )

    return node_names, arcs, timetables, batches


# 路径库构建


def build_graph(arcs: List[Arc]) -> Dict[str, List[Tuple[str, Arc]]]:
    g: Dict[str, List[Tuple[str, Arc]]] = {}
    for a in arcs:
        g.setdefault(a.from_node, []).append((a.to_node, a))
    return g


def random_dfs_paths(graph, origin, dest,
                     max_len=8, max_paths=30) -> List[List[Arc]]:

    paths: List[List[Arc]] = []

    def dfs(node, cur):
        if len(paths) >= max_paths or len(cur) > max_len:
            return
        if node == dest and cur:
            paths.append(cur.copy())
            return
        for nxt, arc in graph.get(node, []):
            if any(a.to_node == nxt for a in cur):
                continue
            if random.random() < 0.1:
                continue
            dfs(nxt, cur + [arc])

    dfs(origin, [])
    return paths


def build_path_library(node_names, arcs, batches, timetables) -> Dict[Tuple[str, str], List[Path]]:
    graph = build_graph(arcs)
    path_lib: Dict[Tuple[str, str], List[Path]] = {}
    next_path_id = 0

    for batch in batches:
        od = (batch.origin, batch.destination)
        if od in path_lib:
            continue

        arc_paths = random_dfs_paths(graph, batch.origin, batch.destination,
                                     max_len=8, max_paths=30)
        paths_for_od: List[Path] = []

        for arc_seq in arc_paths:
            if not arc_seq:
                continue
            nodes = [arc_seq[0].from_node] + [a.to_node for a in arc_seq]
            modes = [a.mode for a in arc_seq]
            base_cost = sum(a.cost_per_teu_km * a.distance for a in arc_seq)
            base_emission = sum(a.emission_per_teu_km * a.distance for a in arc_seq)
            base_time = sum(a.distance / max(a.speed_kmh, 1.0) for a in arc_seq)

            paths_for_od.append(
                Path(
                    path_id=next_path_id,
                    origin=batch.origin,
                    destination=batch.destination,
                    nodes=nodes,
                    modes=modes,
                    arcs=arc_seq,
                    base_cost_per_teu=base_cost,
                    base_emission_per_teu=base_emission,
                    base_travel_time_h=base_time,
                )
            )
            next_path_id += 1

        path_lib[od] = paths_for_od

    return path_lib


# 下层调度：时刻表 + 时间分桶容量统计


def get_timetable_entries(timetables: List[TimetableEntry],
                          from_node: str, to_node: str, mode: str) -> List[TimetableEntry]:
    return [
        t for t in timetables
        if t.from_node == from_node and t.to_node == to_node and t.mode == mode
    ]


def simulate_path_with_timetable_and_capacity(path: Path,
                                              batch: Batch,
                                              flow_teu: float,
                                              timetables: List[TimetableEntry],
                                              bucket_size: float,
                                              arc_flow_by_time: Dict[Tuple[str, str, str, int], float],
                                              arc_slot_user_flow: Dict[Tuple[str, str, str, int, int, int], float]) -> float:
    """
    路径时间仿真，并在时间分桶上累积流量。
    """
    t = batch.ET

    for arc in path.arcs:
        entries = get_timetable_entries(timetables, arc.from_node, arc.to_node, arc.mode)

        if not entries:
            dep = t
        else:
            e = entries[0]
            fd = e.first_departure_hour
            hd = e.headway_hours
            if hd <= 0:
                hd = 24.0
            if t <= fd:
                dep = fd
            else:
                n = math.ceil((t - fd) / hd)
                dep = fd + n * hd
            if dep < t:
                dep = t

        travel = arc.distance / max(arc.speed_kmh, 1.0)
        arr = dep + travel

        start_slot = int(dep // bucket_size)
        end_slot = int((arr - 1e-6) // bucket_size)
        for slot in range(start_slot, end_slot + 1):
            key = (arc.from_node, arc.to_node, arc.mode, slot)
            arc_flow_by_time[key] = arc_flow_by_time.get(key, 0.0) + flow_teu

            user_key = (arc.from_node, arc.to_node, arc.mode, slot,
                        batch.batch_id, path.path_id)
            arc_slot_user_flow[user_key] = arc_slot_user_flow.get(user_key, 0.0) + flow_teu

        t = arr

    return t - batch.ET


def build_arc_index(arcs: List[Arc]) -> Dict[Tuple[str, str, str], Arc]:
    return {(a.from_node, a.to_node, a.mode): a for a in arcs}


# 适应度计算


def evaluate_individual(ind: Individual,
                        batches: List[Batch],
                        path_lib: Dict[Tuple[str, str], List[Path]],
                        arcs: List[Arc],
                        timetables: List[TimetableEntry]):

    arc_index = build_arc_index(arcs)
    arc_flow_by_time: Dict[Tuple[str, str, str, int], float] = {}
    arc_slot_user_flow: Dict[Tuple[str, str, str, int, int, int], float] = {}

    total_cost = 0.0
    total_emission = 0.0
    makespan = 0.0
    path_tardy: Dict[Tuple[int, int], bool] = {}

    for batch in batches:
        od = (batch.origin, batch.destination)
        allocs = ind.od_allocations.get(od, [])
        if not allocs:
            total_cost += 1e10
            total_emission += 1e10
            makespan += 1e6
            continue

        paths_for_od = path_lib.get(od, [])
        if not paths_for_od:
            total_cost += 1e10
            total_emission += 1e10
            makespan += 1e6
            continue

        path_map = {p.path_id: p for p in paths_for_od}
        batch_finish = 0.0

        for alloc in allocs:
            if alloc.share <= 0:
                continue
            if alloc.path_id not in path_map:
                total_cost += 1e10
                total_emission += 1e10
                makespan += 1e6
                continue

            path = path_map[alloc.path_id]
            flow_teu = alloc.share * batch.quantity

            total_cost += path.base_cost_per_teu * flow_teu
            total_emission += path.base_emission_per_teu * flow_teu

            travel_time = simulate_path_with_timetable_and_capacity(
                path, batch, flow_teu, timetables,
                TIME_BUCKET_H, arc_flow_by_time, arc_slot_user_flow
            )
            arrival_time = batch.ET + travel_time
            batch_finish = max(batch_finish, arrival_time)

            if arrival_time > batch.LT:
                path_tardy[(batch.batch_id, path.path_id)] = True

        makespan = max(makespan, batch_finish)

    capacity_violations = []
    arc_slot_to_users: Dict[Tuple[str, str, str, int], List[Tuple[int, int, float]]] = {}
    for (fn, tn, m, slot, bid, pid), flow in arc_slot_user_flow.items():
        key = (fn, tn, m, slot)
        arc_slot_to_users.setdefault(key, []).append((bid, pid, flow))

    for (fn, tn, m, slot), load in arc_flow_by_time.items():
        arc_key = (fn, tn, m)
        if arc_key not in arc_index:
            continue
        arc = arc_index[arc_key]
        cap = arc.capacity
        if load > cap:
            over = load - cap
            users = arc_slot_to_users.get((fn, tn, m, slot), [])
            capacity_violations.append({
                "arc": arc,
                "slot": slot,
                "over": over,
                "user_flows": users
            })

    ind.objectives = (total_cost, total_emission, makespan)
    return capacity_violations, path_tardy


# 变长度编码: 初始化 / 交叉 / 变异 / 修复


def random_initial_individual(batches: List[Batch],
                              path_lib: Dict[Tuple[str, str], List[Path]],
                              max_paths_per_od: int = 4) -> Individual:

    ind = Individual()
    for batch in batches:
        od = (batch.origin, batch.destination)
        paths = path_lib.get(od, [])
        if not paths:
            ind.od_allocations[od] = []
            continue
        L = random.randint(1, min(max_paths_per_od, len(paths)))
        chosen = random.sample(paths, L)
        rand_vals = np.random.rand(L)
        shares = rand_vals / rand_vals.sum()
        ind.od_allocations[od] = [
            PathAllocation(p.path_id, float(s))
            for p, s in zip(chosen, shares)
        ]
    return ind


def fix_shares_for_all_ods(ind: Individual):

    for od, allocs in list(ind.od_allocations.items()):
        for a in allocs:
            if a.share < 0:
                a.share = 0.0
        allocs = [a for a in allocs if a.share > 1e-9]
        if not allocs:
            ind.od_allocations[od] = []
            continue
        ssum = sum(a.share for a in allocs)
        if ssum <= 0:
            allocs[0].share = 1.0
            allocs = [allocs[0]]
        else:
            for a in allocs:
                a.share = a.share / ssum
        ind.od_allocations[od] = allocs


def crossover_paths_for_od(allocs1: List[PathAllocation],
                           allocs2: List[PathAllocation]) -> Tuple[List[PathAllocation], List[PathAllocation]]:

    if not allocs1 and not allocs2:
        return [], []
    if not allocs1:
        return allocs2.copy(), []
    if not allocs2:
        return allocs1.copy(), []

    L1, L2 = len(allocs1), len(allocs2)
    L = max(L1, L2)

    placeholder = PathAllocation(path_id=-1, share=0.0)
    a1 = allocs1.copy() + [placeholder] * (L - L1)
    a2 = allocs2.copy() + [placeholder] * (L - L2)

    cut = random.randint(1, L - 1) if L > 1 else 1
    c1 = a1[:cut] + a2[cut:]
    c2 = a2[:cut] + a1[cut:]

    def clean(child: List[PathAllocation]) -> List[PathAllocation]:
        tmp: Dict[int, float] = {}
        for a in child:
            if a.path_id == -1:
                continue
            tmp[a.path_id] = tmp.get(a.path_id, 0.0) + a.share
        return [PathAllocation(pid, sh) for pid, sh in tmp.items() if sh > 1e-9]

    return clean(c1), clean(c2)


def crossover(parent1: Individual, parent2: Individual,
              batches: List[Batch]) -> Tuple[Individual, Individual]:

    child1 = Individual()
    child2 = Individual()
    for batch in batches:
        od = (batch.origin, batch.destination)
        allocs1 = parent1.od_allocations.get(od, [])
        allocs2 = parent2.od_allocations.get(od, [])
        c1_allocs, c2_allocs = crossover_paths_for_od(allocs1, allocs2)
        child1.od_allocations[od] = c1_allocs
        child2.od_allocations[od] = c2_allocs
    fix_shares_for_all_ods(child1)
    fix_shares_for_all_ods(child2)
    return child1, child2


def mutate(ind: Individual, batches: List[Batch],
           path_lib: Dict[Tuple[str, str], List[Path]],
           p_add=0.25, p_del=0.25, p_mod=0.25, p_replace=0.25):

    batch = random.choice(batches)
    od = (batch.origin, batch.destination)
    allocs = ind.od_allocations.get(od, [])
    paths = path_lib.get(od, [])
    if not paths:
        return
    if not allocs:
        p = random.choice(paths)
        ind.od_allocations[od] = [PathAllocation(p.path_id, 1.0)]
        return

    op = random.random()
    cum_add = p_add
    cum_del = cum_add + p_del
    cum_mod = cum_del + p_mod

    if op < cum_add:
        used = {a.path_id for a in allocs}
        candidates = [p for p in paths if p.path_id not in used]
        if candidates:
            new_p = random.choice(candidates)
            delta = 0.1
            for a in allocs:
                a.share *= (1 - delta)
            allocs.append(PathAllocation(new_p.path_id, delta))

    elif op < cum_del:
        if len(allocs) > 1:
            rm = random.choice(allocs)
            allocs = [a for a in allocs if a.path_id != rm.path_id]

    elif op < cum_mod:
        if len(allocs) >= 2:
            a1, a2 = random.sample(allocs, 2)
            max_delta = 0.3 * a1.share
            if max_delta > 0:
                delta = random.random() * max_delta
                a1.share -= delta
                a2.share += delta

    else:
        used = {a.path_id for a in allocs}
        if len(used) < len(paths):
            victim = random.choice(allocs)
            candidates = [p for p in paths if p.path_id not in used]
            if candidates:
                new_p = random.choice(candidates)
                victim.path_id = new_p.path_id

        if allocs:
            L = len(allocs)
            alphas = np.ones(L)
            new_shares = np.random.dirichlet(alphas)
            for a, s in zip(allocs, new_shares):
                a.share = float(s)

    ind.od_allocations[od] = allocs
    fix_shares_for_all_ods(ind)


def repair(ind: Individual,
           batches: List[Batch],
           path_lib: Dict[Tuple[str, str], List[Path]],
           arcs: List[Arc],
           timetables: List[TimetableEntry]):

    fix_shares_for_all_ods(ind)
    capacity_violations, path_tardy = evaluate_individual(ind, batches, path_lib, arcs, timetables)

    for v in capacity_violations:
        arc = v["arc"]
        slot = v["slot"]
        over = v["over"]
        user_flows = v["user_flows"]

        total_slot_flow = sum(f for (_, _, f) in user_flows)
        if total_slot_flow <= 0:
            continue

        target = total_slot_flow - over
        scale = max(target / total_slot_flow, 0.0)

        for batch_id, path_id, _ in user_flows:
            batch = next(b for b in batches if b.batch_id == batch_id)
            od = (batch.origin, batch.destination)
            allocs = ind.od_allocations.get(od, [])
            for a in allocs:
                if a.path_id == path_id:
                    a.share *= scale

    if path_tardy:
        for (batch_id, path_id) in list(path_tardy.keys()):
            batch = next(b for b in batches if b.batch_id == batch_id)
            od = (batch.origin, batch.destination)
            allocs = ind.od_allocations.get(od, [])
            if not allocs:
                continue
            late_alloc = None
            for a in allocs:
                if a.path_id == path_id:
                    late_alloc = a
                    break
            if late_alloc is None:
                continue

            paths_for_od = path_lib.get(od, [])
            path_map = {p.path_id: p for p in paths_for_od}
            feas = []
            for a in allocs:
                if a.path_id == path_id:
                    continue
                if (batch_id, a.path_id) in path_tardy:
                    continue
                if a.path_id in path_map:
                    feas.append(a)

            if not feas:
                continue

            delta = late_alloc.share
            late_alloc.share = 0.0
            add = delta / len(feas)
            for a in feas:
                a.share += add
            ind.od_allocations[od] = allocs

    fix_shares_for_all_ods(ind)


# NSGA-II 


def dominates(a: Individual, b: Individual) -> bool:
    fa, fb = a.objectives, b.objectives
    return all(x <= y for x, y in zip(fa, fb)) and any(x < y for x, y in zip(fa, fb))


def non_dominated_sort(pop: List[Individual]) -> List[List[Individual]]:
    S = {}
    n = {}
    fronts: List[List[Individual]] = [[]]

    for p in pop:
        S[p] = []
        n[p] = 0
        for q in pop:
            if p is q:
                continue
            if dominates(p, q):
                S[p].append(q)
            elif dominates(q, p):
                n[p] += 1
        if n[p] == 0:
            fronts[0].append(p)

    i = 0
    while fronts[i]:
        nxt = []
        for p in fronts[i]:
            for q in S[p]:
                n[q] -= 1
                if n[q] == 0:
                    nxt.append(q)
        i += 1
        fronts.append(nxt)

    if not fronts[-1]:
        fronts.pop()
    return fronts


def crowding_distance(front: List[Individual]):
    l = len(front)
    if l == 0:
        return {}
    if l == 1:
        return {front[0]: float("inf")}
    distances = {ind: 0.0 for ind in front}
    m = len(front[0].objectives)

    for k in range(m):
        front.sort(key=lambda ind: ind.objectives[k])
        distances[front[0]] = distances[front[-1]] = float("inf")
        fmin = front[0].objectives[k]
        fmax = front[-1].objectives[k]
        if fmax == fmin:
            continue
        for i in range(1, l - 1):
            prev = front[i - 1].objectives[k]
            nxt = front[i + 1].objectives[k]
            distances[front[i]] += (nxt - prev) / (fmax - fmin)
    return distances


def tournament_select(pop: List[Individual],
                      distances: Dict[Individual, float],
                      ranks: Dict[Individual, int]) -> Individual:
    a, b = random.sample(pop, 2)
    if ranks[a] < ranks[b]:
        return a
    if ranks[b] < ranks[a]:
        return b
    if distances[a] > distances[b]:
        return a
    return b


# 初始种群


def build_initial_population(batches, path_lib, arcs, timetables,
                             big_pool_size=400,
                             top_k_cost=80,
                             final_pop_size=30) -> List[Individual]:

    big_pool: List[Individual] = []
    for _ in range(big_pool_size):
        ind = random_initial_individual(batches, path_lib)
        evaluate_individual(ind, batches, path_lib, arcs, timetables)
        big_pool.append(ind)

    big_pool.sort(key=lambda ind: ind.objectives[0])
    cost_filtered = big_pool[:top_k_cost]

    def dominates_2D(a: Individual, b: Individual) -> bool:
        f2a, f3a = a.objectives[1], a.objectives[2]
        f2b, f3b = b.objectives[1], b.objectives[2]
        return (f2a <= f2b and f3a <= f3b) and (f2a < f2b or f3a < f3b)

    good: List[Individual] = []
    for i, s in enumerate(cost_filtered):
        dom_by_others = False
        for j, t in enumerate(cost_filtered):
            if i == j:
                continue
            if dominates_2D(t, s):
                dom_by_others = True
                break
        if not dom_by_others:
            good.append(s)

    pop: List[Individual] = good.copy()
    while len(pop) < final_pop_size and good:
        base = random.choice(good)
        new_ind = Individual(
            od_allocations={od: [PathAllocation(a.path_id, a.share)
                                 for a in allocs]
                            for od, allocs in base.od_allocations.items()}
        )
        mutate(new_ind, batches, path_lib)
        repair(new_ind, batches, path_lib, arcs, timetables)
        pop.append(new_ind)

    if len(pop) > final_pop_size:
        pop = random.sample(pop, final_pop_size)

    return pop


# 输出解的路径拆分


def print_solution_detail(ind: Individual,
                          batches: List[Batch],
                          path_lib: Dict[Tuple[str, str], List[Path]],
                          sol_name: str = "Solution",
                          file=None):
    """
    输出个体在各 OD 上的路径拆分结构。
    """

    def out(line: str = ""):
        # 如果提供 file，只写文件；否则只打印
        if file is not None:
            file.write(line + "\n")
        else:
            print(line)

    out(f"\n===== {sol_name} 详细路径拆分 =====")
    for batch in batches:
        od = (batch.origin, batch.destination)
        allocs = ind.od_allocations.get(od, [])
        allocs = [a for a in allocs if a.share > 1e-6]
        if not allocs:
            continue

        paths_for_od = path_lib.get(od, [])
        path_map = {p.path_id: p for p in paths_for_od}

        out(f"\nOD: {batch.origin} -> {batch.destination}, "
            f"Quantity={batch.quantity}, 路径条数={len(allocs)}")
        if len(allocs) == 1:
            out("  是否发生拆分: 否")
        else:
            out("  是否发生拆分: 是")

        for a in allocs:
            p = path_map.get(a.path_id)
            if p is None:
                continue
            node_str = " -> ".join(p.nodes)
            out(f"  share = {a.share:.3f}, path_id = {p.path_id}")
            out(f"    节点序列: {node_str}")
            out(f"    模式序列: {p.modes}")


# 主函数: NSGA-II


def run_nsga2(filename="extended.xlsx",
              pop_size=30,
              generations=10,
              pc=0.9,
              pm=0.3):
    node_names, arcs, timetables, batches = load_network_from_extended(filename)
    path_lib = build_path_library(node_names, arcs, batches, timetables)

    population = build_initial_population(
        batches, path_lib, arcs, timetables,
        big_pool_size=400,
        top_k_cost=80,
        final_pop_size=pop_size
    )

    for ind in population:
        repair(ind, batches, path_lib, arcs, timetables)

    for gen in range(generations):
        for ind in population:
            evaluate_individual(ind, batches, path_lib, arcs, timetables)

        fronts = non_dominated_sort(population)
        ranks = {}
        for i, front in enumerate(fronts):
            for ind in front:
                ranks[ind] = i

        all_distances = {}
        for front in fronts:
            d = crowding_distance(front)
            all_distances.update(d)

        mating_pool: List[Individual] = []
        while len(mating_pool) < pop_size:
            mating_pool.append(tournament_select(population, all_distances, ranks))

        offspring: List[Individual] = []
        while len(offspring) < pop_size:
            if random.random() < pc:
                p1, p2 = random.sample(mating_pool, 2)
                c1, c2 = crossover(p1, p2, batches)
            else:
                p = random.choice(mating_pool)
                c1 = Individual(
                    od_allocations={od: [PathAllocation(a.path_id, a.share)
                                         for a in allocs]
                                    for od, allocs in p.od_allocations.items()}
                )
                c2 = Individual(
                    od_allocations={od: [PathAllocation(a.path_id, a.share)
                                         for a in allocs]
                                    for od, allocs in p.od_allocations.items()}
                )

            if random.random() < pm:
                mutate(c1, batches, path_lib)
            if random.random() < pm:
                mutate(c2, batches, path_lib)

            repair(c1, batches, path_lib, arcs, timetables)
            repair(c2, batches, path_lib, arcs, timetables)

            offspring.append(c1)
            if len(offspring) < pop_size:
                offspring.append(c2)

        union = population + offspring
        for ind in union:
            evaluate_individual(ind, batches, path_lib, arcs, timetables)

        fronts = non_dominated_sort(union)
        new_pop: List[Individual] = []
        for front in fronts:
            if len(new_pop) + len(front) <= pop_size:
                new_pop.extend(front)
            else:
                d = crowding_distance(front)
                sorted_front = sorted(front, key=lambda x: d[x], reverse=True)
                new_pop.extend(sorted_front[:pop_size - len(new_pop)])
                break

        population = new_pop

        f1_vals = [ind.objectives[0] for ind in population]
        f2_vals = [ind.objectives[1] for ind in population]
        f3_vals = [ind.objectives[2] for ind in population]
        print(f"Gen {gen}: f1(min,mean) = {min(f1_vals):.2f}, {np.mean(f1_vals):.2f}; "
              f"f2(min) = {min(f2_vals):.2f}; f3(min) = {min(f3_vals):.2f}")

    fronts = non_dominated_sort(population)
    pareto = fronts[0]

    print("\nFinal Pareto front:")
    for i, ind in enumerate(pareto):
        f1, f2, f3 = ind.objectives
        print(f"Sol {i}: cost={f1:.2f}, emission={f2:.2f}, time={f3:.2f}")

    # 屏幕打印前 3 个解的详细路径
    for i, ind in enumerate(pareto[:3]):
        print_solution_detail(ind, batches, path_lib, sol_name=f"Pareto 解 {i}")

    # 将结果单独保存到一个 txt 文件中
    with open("results.txt", "w", encoding="utf-8") as f:
        f.write("Final Pareto front:\n")
        for i, ind in enumerate(pareto):
            f1, f2, f3 = ind.objectives
            line = f"Sol {i}: cost={f1:.2f}, emission={f2:.2f}, time={f3:.2f}"
            f.write(line + "\n")

        for i, ind in enumerate(pareto[:3]):
            print_solution_detail(ind, batches, path_lib,
                                  sol_name=f"Pareto 解 {i}", file=f)

    return pareto, population, path_lib, batches


if __name__ == "__main__":
    pareto, population, path_lib, batches = run_nsga2(
        filename="extended.xlsx",
        pop_size=30,
        generations=10,
        pc=0.9,
        pm=0.3,
    )
