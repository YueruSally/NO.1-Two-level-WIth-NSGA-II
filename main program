import math
import random
from dataclasses import dataclass, field
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# ========================
# 全局设置
# ========================

# 时间分桶大小（小时）
TIME_BUCKET_H = 1.0

# 区域标签集合（需要与你的 Nodes 表中 Region 列保持一致，可按需要修改）
CHINA_REGIONS = {"CN", "China"}
EUROPE_REGIONS = {"WE", "EE", "EU", "Europe"}


# ========================
# 数据结构定义
# ========================

@dataclass
class Arc:
    from_node: str
    to_node: str
    mode: str
    distance: float
    capacity: float              # 每个时间段容量（TEU）
    cost_per_teu_km: float
    emission_per_teu_km: float
    speed_kmh: float


@dataclass
class TimetableEntry:
    from_node: str
    to_node: str
    mode: str
    frequency_per_week: float
    first_departure_hour: float
    headway_hours: float


@dataclass
class Batch:
    batch_id: int
    origin: str
    destination: str
    quantity: float
    ET: float
    LT: float


@dataclass
class Path:
    path_id: int
    origin: str
    destination: str
    nodes: List[str]
    modes: List[str]
    arcs: List[Arc]
    base_cost_per_teu: float
    base_emission_per_teu: float
    base_travel_time_h: float


@dataclass
class PathAllocation:
    path_id: int
    share: float


@dataclass(eq=False)
class Individual:
    # 变长度编码：每个 OD 对应一个可变长度路径集合
    od_allocations: Dict[Tuple[str, str], List[PathAllocation]] = field(default_factory=dict)
    objectives: Tuple[float, float, float] = (float("inf"), float("inf"), float("inf"))


# ========================
# Excel 数据读取
# ========================

def load_network_from_extended(filename: str):
    """
    从 Excel 文件中读取 Nodes, Arcs_All, Timetable, Batches。
    """
    xls = pd.ExcelFile(filename)

    # Nodes
    if "Nodes" not in xls.sheet_names:
        raise ValueError(f"{filename} 中必须包含 Nodes 表")
    nodes_df = pd.read_excel(xls, "Nodes")
    for c in ["EnglishName", "Region"]:
        if c not in nodes_df.columns:
            raise ValueError(f"Nodes 表中缺少列: {c}")

    node_names = nodes_df["EnglishName"].astype(str).tolist()
    node_region = dict(
        zip(
            nodes_df["EnglishName"].astype(str),
            nodes_df["Region"].astype(str),
        )
    )

    # Arcs_All
    if "Arcs_All" not in xls.sheet_names:
        raise ValueError(f"{filename} 中必须包含 Arcs_All 表")
    arcs_df = pd.read_excel(xls, "Arcs_All")
    required_arc_cols = [
        "OriginEN", "DestEN", "Mode",
        "Distance_km", "Cost_$_per_km", "Emission_gCO2_per_tkm"
    ]
    for c in required_arc_cols:
        if c not in arcs_df.columns:
            raise ValueError(f"Arcs_All 表中缺少列: {c}")

    has_cap_col = "Capacity_TEU" in arcs_df.columns

    arcs: List[Arc] = []
    for _, row in arcs_df.iterrows():
        mode_raw = str(row["Mode"]).strip().lower()
        if mode_raw == "rail":
            mode = "rail"
            speed = 50.0
        elif mode_raw == "road":
            mode = "road"
            speed = 75.0
        elif mode_raw == "water":
            mode = "water"
            speed = 30.0
        else:
            mode = mode_raw
            speed = 50.0

        dist_str = str(row["Distance_km"])
        cleaned = "".join(ch for ch in dist_str if (ch.isdigit() or ch == "."))
        distance = float(cleaned) if cleaned else 0.0

        cost = float(row["Cost_$_per_km"])
        emis = float(row["Emission_gCO2_per_tkm"])

        if has_cap_col and not pd.isna(row["Capacity_TEU"]):
            capacity = float(row["Capacity_TEU"])
        else:
            capacity = 1e9

        arcs.append(
            Arc(
                from_node=str(row["OriginEN"]).strip(),
                to_node=str(row["DestEN"]).strip(),
                mode=mode,
                distance=distance,
                capacity=capacity,
                cost_per_teu_km=cost,
                emission_per_teu_km=emis,
                speed_kmh=speed,
            )
        )

    # Timetable
    if "Timetable" not in xls.sheet_names:
        raise ValueError(f"{filename} 中必须包含 Timetable 表")
    tdf = pd.read_excel(xls, "Timetable")
    required_tt_cols = [
        "OriginEN", "DestEN", "Mode",
        "Frequency_per_week", "FirstDepartureHour", "Headway_Hours"
    ]
    for c in required_tt_cols:
        if c not in tdf.columns:
            raise ValueError(f"Timetable 表中缺少列: {c}")

    timetables: List[TimetableEntry] = []
    for _, row in tdf.iterrows():
        mode_raw = str(row["Mode"]).strip().lower()
        if mode_raw == "rail":
            mode = "rail"
        elif mode_raw == "road":
            mode = "road"
        elif mode_raw == "water":
            mode = "water"
        else:
            mode = mode_raw

        freq = float(row["Frequency_per_week"])

        v = row["FirstDepartureHour"]
        if pd.isna(v):
            fd = 0.0
        else:
            s = str(v)
            if ":" in s:
                try:
                    fd = float(s.split(":")[0])
                except Exception:
                    fd = 0.0
            else:
                fd = float(s)

        hd = row["Headway_Hours"]
        if pd.isna(hd):
            hd = 168.0 / max(freq, 1.0)
        else:
            hd = float(hd)

        timetables.append(
            TimetableEntry(
                from_node=str(row["OriginEN"]).strip(),
                to_node=str(row["DestEN"]).strip(),
                mode=mode,
                frequency_per_week=freq,
                first_departure_hour=fd,
                headway_hours=hd,
            )
        )

    # Batches：只保留“起点在中国、终点在欧洲”的 OD
    if "Batches" not in xls.sheet_names:
        raise ValueError(
            f"{filename} 中必须包含 Batches 表，并包含列: "
            "BatchID, OriginEN, DestEN, QuantityTEU, ET, LT"
        )
    bdf = pd.read_excel(xls, "Batches")
    required_b_cols = ["BatchID", "OriginEN", "DestEN", "QuantityTEU", "ET", "LT"]
    for c in required_b_cols:
        if c not in bdf.columns:
            raise ValueError(f"Batches 表中缺少列: {c}")

    batches: List[Batch] = []
    for _, row in bdf.iterrows():
        bid = int(row["BatchID"])
        origin = str(row["OriginEN"]).strip()
        dest = str(row["DestEN"]).strip()
        qty = float(row["QuantityTEU"])
        et = float(row["ET"])
        lt = float(row["LT"])

        o_reg = node_region.get(origin)
        d_reg = node_region.get(dest)

        # 核心筛选条件：起点在中国区域，终点在欧洲区域
        if o_reg not in CHINA_REGIONS or d_reg not in EUROPE_REGIONS:
            continue

        batches.append(
            Batch(
                batch_id=bid,
                origin=origin,
                destination=dest,
                quantity=qty,
                ET=et,
                LT=lt,
            )
        )

    return node_names, arcs, timetables, batches


# ========================
# 路径库构建
# ========================

def build_graph(arcs: List[Arc]) -> Dict[str, List[Tuple[str, Arc]]]:
    g: Dict[str, List[Tuple[str, Arc]]] = {}
    for a in arcs:
        g.setdefault(a.from_node, []).append((a.to_node, a))
    return g


def random_dfs_paths(graph, origin, dest,
                     max_len=8, max_paths=30) -> List[List[Arc]]:
    """
    使用 DFS 随机生成不含环路的简单路径：
    - 路径中不允许任何节点重复，包括起点
    """
    paths: List[List[Arc]] = []

    def dfs(node: str,
            cur_arcs: List[Arc],
            visited_nodes: set):
        if len(paths) >= max_paths or len(cur_arcs) > max_len:
            return

        if node == dest and cur_arcs:
            paths.append(cur_arcs.copy())
            return

        for nxt, arc in graph.get(node, []):
            if nxt in visited_nodes:
                continue
            if random.random() < 0.1:
                continue

            dfs(
                nxt,
                cur_arcs + [arc],
                visited_nodes | {nxt}
            )

    dfs(origin, [], {origin})
    return paths


def build_path_library(node_names, arcs, batches, timetables) -> Dict[Tuple[str, str], List[Path]]:
    graph = build_graph(arcs)
    path_lib: Dict[Tuple[str, str], List[Path]] = {}
    next_path_id = 0

    for batch in batches:
        od = (batch.origin, batch.destination)
        if od in path_lib:
            continue

        arc_paths = random_dfs_paths(graph, batch.origin, batch.destination,
                                     max_len=8, max_paths=30)
        paths_for_od: List[Path] = []

        for arc_seq in arc_paths:
            if not arc_seq:
                continue
            nodes = [arc_seq[0].from_node] + [a.to_node for a in arc_seq]
            modes = [a.mode for a in arc_seq]
            base_cost = sum(a.cost_per_teu_km * a.distance for a in arc_seq)
            base_emission = sum(a.emission_per_teu_km * a.distance for a in arc_seq)
            base_time = sum(a.distance / max(a.speed_kmh, 1.0) for a in arc_seq)

            paths_for_od.append(
                Path(
                    path_id=next_path_id,
                    origin=batch.origin,
                    destination=batch.destination,
                    nodes=nodes,
                    modes=modes,
                    arcs=arc_seq,
                    base_cost_per_teu=base_cost,
                    base_emission_per_teu=base_emission,
                    base_travel_time_h=base_time,
                )
            )
            next_path_id += 1

        # 剔除“极度绕路”的路径：按成本过滤 + 限制条数
        if paths_for_od:
            paths_for_od.sort(key=lambda p: p.base_cost_per_teu)
            best_cost = paths_for_od[0].base_cost_per_teu
            cost_factor = 1.3
            filtered = [
                p for p in paths_for_od
                if p.base_cost_per_teu <= cost_factor * best_cost
            ]
            max_keep = 10
            paths_for_od = filtered[:max_keep]

        path_lib[od] = paths_for_od

    return path_lib


# ========================
# 下层调度：时刻表 + 时间分桶容量统计
# ========================

def get_timetable_entries(timetables: List[TimetableEntry],
                          from_node: str, to_node: str, mode: str) -> List[TimetableEntry]:
    return [
        t for t in timetables
        if t.from_node == from_node and t.to_node == to_node and t.mode == mode
    ]


def simulate_path_with_timetable_and_capacity(path: Path,
                                              batch: Batch,
                                              flow_teu: float,
                                              timetables: List[TimetableEntry],
                                              bucket_size: float,
                                              arc_flow_by_time: Dict[Tuple[str, str, str, int], float],
                                              arc_slot_user_flow: Dict[Tuple[str, str, str, int, int, int], float]) -> float:
    """
    路径时间仿真，并在时间分桶上累积流量。
    """
    t = batch.ET

    for arc in path.arcs:
        entries = get_timetable_entries(timetables, arc.from_node, arc.to_node, arc.mode)

        if not entries:
            dep = t
        else:
            e = entries[0]
            fd = e.first_departure_hour
            hd = e.headway_hours
            if hd <= 0:
                hd = 24.0
            if t <= fd:
                dep = fd
            else:
                n = math.ceil((t - fd) / hd)
                dep = fd + n * hd

        travel = arc.distance / max(arc.speed_kmh, 1.0)
        arr = dep + travel

        start_slot = int(dep // bucket_size)
        end_slot = int((arr - 1e-6) // bucket_size)
        for slot in range(start_slot, end_slot + 1):
            key = (arc.from_node, arc.to_node, arc.mode, slot)
            arc_flow_by_time[key] = arc_flow_by_time.get(key, 0.0) + flow_teu

            user_key = (arc.from_node, arc.to_node, arc.mode, slot,
                        batch.batch_id, path.path_id)
            arc_slot_user_flow[user_key] = arc_slot_user_flow.get(user_key, 0.0) + flow_teu

        t = arr

    return t - batch.ET


def simulate_path_dry(path: Path,
                      batch: Batch,
                      flow_teu: float,
                      timetables: List[TimetableEntry],
                      bucket_size: float):
    """
    干跑模拟：返回占用的 arc-slot 列表和到达时间，不写入全局。
    """
    t = batch.ET
    touched: List[Tuple[Tuple[str, str, str, int], float]] = []

    for arc in path.arcs:
        entries = get_timetable_entries(timetables, arc.from_node, arc.to_node, arc.mode)

        if not entries:
            dep = t
        else:
            e = entries[0]
            fd = e.first_departure_hour
            hd = e.headway_hours if e.headway_hours > 0 else 24.0
            if t <= fd:
                dep = fd
            else:
                n = math.ceil((t - fd) / hd)
                dep = fd + n * hd

        travel = arc.distance / max(arc.speed_kmh, 1.0)
        arr = dep + travel

        start_slot = int(dep // bucket_size)
        end_slot = int((arr - 1e-6) // bucket_size)
        for slot in range(start_slot, end_slot + 1):
            touched.append(((arc.from_node, arc.to_node, arc.mode, slot), flow_teu))

        t = arr

    arrival_time = t
    return touched, arrival_time


def build_arc_index(arcs: List[Arc]) -> Dict[Tuple[str, str, str], Arc]:
    return {(a.from_node, a.to_node, a.mode): a for a in arcs}


def try_commit_flow(path: Path,
                    batch: Batch,
                    flow_teu: float,
                    timetables: List[TimetableEntry],
                    bucket_size: float,
                    arc_flow_by_time: Dict[Tuple[str, str, str, int], float],
                    arc_slot_user_flow: Dict[Tuple[str, str, str, int, int, int], float],
                    arc_index: Dict[Tuple[str, str, str], Arc]) -> bool:
    """
    先干跑拿到占用，再检查容量和 LT，若通过则写入全局结构。
    """
    touched, arrival_time = simulate_path_dry(
        path, batch, flow_teu, timetables, bucket_size
    )

    if arrival_time > batch.LT:
        return False

    # 容量检查
    for (fn, tn, m, slot), add in touched:
        arc = arc_index.get((fn, tn, m))
        if arc is None:
            return False
        cur = arc_flow_by_time.get((fn, tn, m, slot), 0.0)
        if cur + add > arc.capacity + 1e-9:
            return False

    # 真正写入
    for (fn, tn, m, slot), add in touched:
        key = (fn, tn, m, slot)
        arc_flow_by_time[key] = arc_flow_by_time.get(key, 0.0) + add
        user_key = (fn, tn, m, slot, batch.batch_id, path.path_id)
        arc_slot_user_flow[user_key] = arc_slot_user_flow.get(user_key, 0.0) + add

    return True


# ========================
# 适应度计算
# ========================

def evaluate_individual(ind: Individual,
                        batches: List[Batch],
                        path_lib: Dict[Tuple[str, str], List[Path]],
                        arcs: List[Arc],
                        timetables: List[TimetableEntry]):

    arc_index = build_arc_index(arcs)
    arc_flow_by_time: Dict[Tuple[str, str, str, int], float] = {}
    arc_slot_user_flow: Dict[Tuple[str, str, str, int, int, int], float] = {}

    total_cost = 0.0
    total_emission = 0.0
    makespan = 0.0
    path_tardy: Dict[Tuple[int, int], bool] = {}

    for batch in batches:
        od = (batch.origin, batch.destination)
        allocs = ind.od_allocations.get(od, [])
        if not allocs:
            total_cost += 1e10
            total_emission += 1e10
            makespan += 1e6
            continue

        paths_for_od = path_lib.get(od, [])
        if not paths_for_od:
            total_cost += 1e10
            total_emission += 1e10
            makespan += 1e6
            continue

        path_map = {p.path_id: p for p in paths_for_od}
        batch_finish = 0.0

        for alloc in allocs:
            if alloc.share <= 0:
                continue
            if alloc.path_id not in path_map:
                total_cost += 1e10
                total_emission += 1e10
                makespan += 1e6
                continue

            path = path_map[alloc.path_id]
            flow_teu = alloc.share * batch.quantity

            total_cost += path.base_cost_per_teu * flow_teu
            total_emission += path.base_emission_per_teu * flow_teu

            travel_time = simulate_path_with_timetable_and_capacity(
                path, batch, flow_teu, timetables,
                TIME_BUCKET_H, arc_flow_by_time, arc_slot_user_flow
            )
            arrival_time = batch.ET + travel_time
            batch_finish = max(batch_finish, arrival_time)

            if arrival_time > batch.LT:
                path_tardy[(batch.batch_id, path.path_id)] = True

        makespan = max(makespan, batch_finish)

    capacity_violations = []
    arc_slot_to_users: Dict[Tuple[str, str, str, int], List[Tuple[int, int, float]]] = {}
    for (fn, tn, m, slot, bid, pid), flow in arc_slot_user_flow.items():
        key = (fn, tn, m, slot)
        arc_slot_to_users.setdefault(key, []).append((bid, pid, flow))

    for (fn, tn, m, slot), load in arc_flow_by_time.items():
        arc = arc_index.get((fn, tn, m))
        if arc is None:
            continue
        cap = arc.capacity
        if load > cap:
            over = load - cap
            users = arc_slot_to_users.get((fn, tn, m, slot), [])
            capacity_violations.append({
                "arc": arc,
                "slot": slot,
                "over": over,
                "user_flows": users
            })

    ind.objectives = (total_cost, total_emission, makespan)
    return capacity_violations, path_tardy


# ========================
# 工具：share 离散化
# ========================

def discretize_shares_for_all_ods(ind: Individual, step: float = 0.1):
    """
    将每个 OD 上的路径 share 离散化到 {0, step, 2*step, ..., 1}，且和为 1。
    默认 step=0.1。
    """
    if step <= 0 or step > 1:
        return
    M = int(round(1.0 / step))

    for od, allocs in ind.od_allocations.items():
        if not allocs:
            continue

        raw = [max(a.share, 0.0) for a in allocs]
        total = sum(raw)
        if total <= 0:
            raw = [1.0] * len(allocs)
            total = float(len(allocs))
        norm = [r / total for r in raw]

        base_counts = [int(math.floor(s * M)) for s in norm]
        sum_base = sum(base_counts)
        remainder = M - sum_base

        fracs = [(norm[i] * M - base_counts[i], i) for i in range(len(allocs))]
        fracs.sort(reverse=True)
        for k in range(remainder):
            idx = fracs[k % len(allocs)][1]
            base_counts[idx] += 1

        shares = [c / M for c in base_counts]
        for a, sh in zip(allocs, shares):
            a.share = sh


# ========================
# 变长度编码: 初始化 / 交叉 / 变异 / 修复
# ========================

def random_initial_individual(batches: List[Batch],
                              path_lib: Dict[Tuple[str, str], List[Path]],
                              max_paths_per_od: int = 4) -> Individual:

    ind = Individual()
    for batch in batches:
        od = (batch.origin, batch.destination)
        paths = path_lib.get(od, [])
        if not paths:
            ind.od_allocations[od] = []
            continue
        L = random.randint(1, min(max_paths_per_od, len(paths)))
        chosen = random.sample(paths, L)
        rand_vals = np.random.rand(L)
        shares = rand_vals / rand_vals.sum()
        ind.od_allocations[od] = [
            PathAllocation(p.path_id, float(s))
            for p, s in zip(chosen, shares)
        ]

    discretize_shares_for_all_ods(ind, step=0.1)
    return ind


def fix_shares_for_all_ods(ind: Individual):

    for od, allocs in list(ind.od_allocations.items()):
        for a in allocs:
            if a.share < 0:
                a.share = 0.0
        allocs = [a for a in allocs if a.share > 1e-9]
        if not allocs:
            ind.od_allocations[od] = []
            continue
        ssum = sum(a.share for a in allocs)
        if ssum <= 0:
            allocs[0].share = 1.0
            allocs = [allocs[0]]
        else:
            for a in allocs:
                a.share = a.share / ssum
        ind.od_allocations[od] = allocs


def crossover_paths_for_od(allocs1: List[PathAllocation],
                           allocs2: List[PathAllocation]) -> Tuple[List[PathAllocation], List[PathAllocation]]:

    if not allocs1 and not allocs2:
        return [], []
    if not allocs1:
        return allocs2.copy(), []
    if not allocs2:
        return allocs1.copy(), []

    L1, L2 = len(allocs1), len(allocs2)
    L = max(L1, L2)

    placeholder = PathAllocation(path_id=-1, share=0.0)
    a1 = allocs1.copy() + [placeholder] * (L - L1)
    a2 = allocs2.copy() + [placeholder] * (L - L2)

    cut = random.randint(1, L - 1) if L > 1 else 1
    c1 = a1[:cut] + a2[cut:]
    c2 = a2[:cut] + a1[cut:]

    def clean(child: List[PathAllocation]) -> List[PathAllocation]:
        tmp: Dict[int, float] = {}
        for a in child:
            if a.path_id == -1:
                continue
            tmp[a.path_id] = tmp.get(a.path_id, 0.0) + a.share
        return [PathAllocation(pid, sh) for pid, sh in tmp.items() if sh > 1e-9]

    return clean(c1), clean(c2)


def crossover(parent1: Individual, parent2: Individual,
              batches: List[Batch]) -> Tuple[Individual, Individual]:

    child1 = Individual()
    child2 = Individual()
    for batch in batches:
        od = (batch.origin, batch.destination)
        allocs1 = parent1.od_allocations.get(od, [])
        allocs2 = parent2.od_allocations.get(od, [])
        c1_allocs, c2_allocs = crossover_paths_for_od(allocs1, allocs2)
        child1.od_allocations[od] = c1_allocs
        child2.od_allocations[od] = c2_allocs
    fix_shares_for_all_ods(child1)
    fix_shares_for_all_ods(child2)
    discretize_shares_for_all_ods(child1, step=0.1)
    discretize_shares_for_all_ods(child2, step=0.1)
    return child1, child2


def mutate(ind: Individual, batches: List[Batch],
           path_lib: Dict[Tuple[str, str], List[Path]],
           p_add=0.25, p_del=0.25, p_mod=0.25, p_replace=0.25):

    batch = random.choice(batches)
    od = (batch.origin, batch.destination)
    allocs = ind.od_allocations.get(od, [])
    paths = path_lib.get(od, [])
    if not paths:
        return
    if not allocs:
        p = random.choice(paths)
        ind.od_allocations[od] = [PathAllocation(p.path_id, 1.0)]
        discretize_shares_for_all_ods(ind, step=0.1)
        return

    op = random.random()
    cum_add = p_add
    cum_del = cum_add + p_del
    cum_mod = cum_del + p_mod

    if op < cum_add:
        used = {a.path_id for a in allocs}
        candidates = [p for p in paths if p.path_id not in used]
        if candidates:
            new_p = random.choice(candidates)
            delta = 0.1
            for a in allocs:
                a.share *= (1 - delta)
            allocs.append(PathAllocation(new_p.path_id, delta))

    elif op < cum_del:
        if len(allocs) > 1:
            rm = random.choice(allocs)
            allocs = [a for a in allocs if a.path_id != rm.path_id]

    elif op < cum_mod:
        if len(allocs) >= 2:
            a1, a2 = random.sample(allocs, 2)
            max_delta = 0.3 * a1.share
            if max_delta > 0:
                delta = random.random() * max_delta
                a1.share -= delta
                a2.share += delta

    else:
        used = {a.path_id for a in allocs}
        if len(used) < len(paths):
            victim = random.choice(allocs)
            candidates = [p for p in paths if p.path_id not in used]
            if candidates:
                new_p = random.choice(candidates)
                victim.path_id = new_p.path_id

        if allocs:
            L = len(allocs)
            alphas = np.ones(L)
            new_shares = np.random.dirichlet(alphas)
            for a, s in zip(allocs, new_shares):
                a.share = float(s)

    ind.od_allocations[od] = allocs
    fix_shares_for_all_ods(ind)
    discretize_shares_for_all_ods(ind, step=0.1)


def repair(ind: Individual,
           batches: List[Batch],
           path_lib: Dict[Tuple[str, str], List[Path]],
           arcs: List[Arc],
           timetables: List[TimetableEntry]):

    fix_shares_for_all_ods(ind)
    capacity_violations, path_tardy = evaluate_individual(ind, batches, path_lib, arcs, timetables)

    for v in capacity_violations:
        arc = v["arc"]
        slot = v["slot"]
        over = v["over"]
        user_flows = v["user_flows"]

        total_slot_flow = sum(f for (_, _, f) in user_flows)
        if total_slot_flow <= 0:
            continue

        target = total_slot_flow - over
        scale = max(target / total_slot_flow, 0.0)

        for batch_id, path_id, _ in user_flows:
            batch = next(b for b in batches if b.batch_id == batch_id)
            od = (batch.origin, batch.destination)
            allocs = ind.od_allocations.get(od, [])
            for a in allocs:
                if a.path_id == path_id:
                    a.share *= scale

    if path_tardy:
        for (batch_id, path_id) in list(path_tardy.keys()):
            batch = next(b for b in batches if b.batch_id == batch_id)
            od = (batch.origin, batch.destination)
            allocs = ind.od_allocations.get(od, [])
            if not allocs:
                continue
            late_alloc = None
            for a in allocs:
                if a.path_id == path_id:
                    late_alloc = a
                    break
            if late_alloc is None:
                continue

            paths_for_od = path_lib.get(od, [])
            path_map = {p.path_id: p for p in paths_for_od}
            feas = []
            for a in allocs:
                if a.path_id == path_id:
                    continue
                if (batch_id, a.path_id) in path_tardy:
                    continue
                if a.path_id in path_map:
                    feas.append(a)

            if not feas:
                continue

            delta = late_alloc.share
            late_alloc.share = 0.0
            add = delta / len(feas)
            for a in feas:
                a.share += add
            ind.od_allocations[od] = allocs

    fix_shares_for_all_ods(ind)
    discretize_shares_for_all_ods(ind, step=0.1)


# ========================
# NSGA-II 基本算子
# ========================

def dominates(a: Individual, b: Individual) -> bool:
    fa, fb = a.objectives, b.objectives
    return all(x <= y for x, y in zip(fa, fb)) and any(x < y for x, y in zip(fa, fb))


def non_dominated_sort(pop: List[Individual]) -> List[List[Individual]]:
    S = {}
    n = {}
    fronts: List[List[Individual]] = [[]]

    for p in pop:
        S[p] = []
        n[p] = 0
        for q in pop:
            if p is q:
                continue
            if dominates(p, q):
                S[p].append(q)
            elif dominates(q, p):
                n[p] += 1
        if n[p] == 0:
            fronts[0].append(p)

    i = 0
    while fronts[i]:
        nxt = []
        for p in fronts[i]:
            for q in S[p]:
                n[q] -= 1
                if n[q] == 0:
                    nxt.append(q)
        i += 1
        fronts.append(nxt)

    if not fronts[-1]:
        fronts.pop()
    return fronts


def crowding_distance(front: List[Individual]):
    l = len(front)
    if l == 0:
        return {}
    if l == 1:
        return {front[0]: float("inf")}
    distances = {ind: 0.0 for ind in front}
    m = len(front[0].objectives)

    for k in range(m):
        front.sort(key=lambda ind: ind.objectives[k])
        distances[front[0]] = distances[front[-1]] = float("inf")
        fmin = front[0].objectives[k]
        fmax = front[-1].objectives[k]
        if fmax == fmin:
            continue
        for i in range(1, l - 1):
            prev = front[i - 1].objectives[k]
            nxt = front[i + 1].objectives[k]
            distances[front[i]] += (nxt - prev) / (fmax - fmin)
    return distances


def tournament_select(pop: List[Individual],
                      distances: Dict[Individual, float],
                      ranks: Dict[Individual, int]) -> Individual:
    a, b = random.sample(pop, 2)
    if ranks[a] < ranks[b]:
        return a
    if ranks[b] < ranks[a]:
        return b
    if distances[a] > distances[b]:
        return a
    return b


# ========================
# 构造式“可行”个体 + 初始种群
# ========================

def constructive_feasible_individual(batches: List[Batch],
                                     path_lib: Dict[Tuple[str, str], List[Path]],
                                     arcs: List[Arc],
                                     timetables: List[TimetableEntry],
                                     share_step: float = 0.1,
                                     topK_paths: int = 8,
                                     cost_slack: float = 0.2) -> Individual:
    """
    构造一个“尽量可行”的个体：
    - 每个 OD 的批次按 share_step 拆成 M 份（M=1/share_step）；
    - 每份优先放到成本较低且可行的路径上（允许 best*(1+cost_slack) 以内随机选择）；
    - 提交前用 try_commit_flow 做容量+LT 检查。
    """
    ind = Individual()
    arc_index = build_arc_index(arcs)
    arc_flow_by_time: Dict[Tuple[str, str, str, int], float] = {}
    arc_slot_user_flow: Dict[Tuple[str, str, str, int, int, int], float] = {}

    if share_step <= 0 or share_step > 1:
        share_step = 0.1
    M = int(round(1.0 / share_step))

    for batch in batches:
        od = (batch.origin, batch.destination)
        paths = path_lib.get(od, [])
        if not paths:
            ind.od_allocations[od] = []
            continue

        # 候选路径：按成本排序，取前 topK，再允许 best*(1+cost_slack) 范围
        paths_sorted = sorted(paths, key=lambda p: p.base_cost_per_teu)
        best_cost = paths_sorted[0].base_cost_per_teu
        cand = [p for p in paths_sorted[:topK_paths]
                if p.base_cost_per_teu <= best_cost * (1.0 + cost_slack)]
        if not cand:
            cand = paths_sorted[:min(topK_paths, len(paths_sorted))]

        share_map: Dict[int, float] = {}
        unit_teu = share_step * batch.quantity

        for _ in range(M):
            random.shuffle(cand)
            placed = False
            for path in cand:
                if try_commit_flow(path, batch, unit_teu, timetables, TIME_BUCKET_H,
                                   arc_flow_by_time, arc_slot_user_flow, arc_index):
                    share_map[path.path_id] = share_map.get(path.path_id, 0.0) + share_step
                    placed = True
                    break
            if not placed:
                # 这份放不进去就丢弃（对应这部分量不可行）
                pass

        total_share = sum(share_map.values())
        if total_share > 0:
            norm = {pid: sh / total_share for pid, sh in share_map.items()}
            ind.od_allocations[od] = [PathAllocation(pid, sh) for pid, sh in norm.items()]
        else:
            ind.od_allocations[od] = []

    # 再做一遍离散化让 shares 精确落在网格上
    discretize_shares_for_all_ods(ind, step=share_step)
    return ind


def build_initial_population(batches, path_lib, arcs, timetables,
                             big_pool_size=400,
                             top_k_cost=80,      # 保留接口，不再使用
                             final_pop_size=30) -> List[Individual]:

    # 1) 先用“可行构造”生成大池子
    big_pool: List[Individual] = []
    for _ in range(big_pool_size):
        ind = constructive_feasible_individual(
            batches, path_lib, arcs, timetables,
            share_step=0.1, topK_paths=8, cost_slack=0.2
        )
        evaluate_individual(ind, batches, path_lib, arcs, timetables)
        big_pool.append(ind)

    # 2) f1/f2/f3 各取前 10
    K = 10
    best_f1 = sorted(big_pool, key=lambda x: x.objectives[0])[:K]
    best_f2 = sorted(big_pool, key=lambda x: x.objectives[1])[:K]
    best_f3 = sorted(big_pool, key=lambda x: x.objectives[2])[:K]

    pop: List[Individual] = []
    for lst in (best_f1, best_f2, best_f3):
        for ind in lst:
            if ind not in pop:
                pop.append(ind)

    # 3) 数量不足，用 big_pool 克隆补齐（不在这里 mutate/repair）
    while len(pop) < final_pop_size:
        src = random.choice(big_pool)
        clone = Individual(
            od_allocations={od: [PathAllocation(a.path_id, a.share)
                                 for a in allocs]
                            for od, allocs in src.od_allocations.items()},
            objectives=src.objectives
        )
        pop.append(clone)

    # 4) 若数量超过，用非支配排序+拥挤度裁剪
    if len(pop) > final_pop_size:
        fronts = non_dominated_sort(pop)
        new_pop: List[Individual] = []
        for front in fronts:
            if len(new_pop) + len(front) <= final_pop_size:
                new_pop.extend(front)
            else:
                d = crowding_distance(front)
                sorted_front = sorted(front, key=lambda x: d[x], reverse=True)
                new_pop.extend(sorted_front[:final_pop_size - len(new_pop)])
                break
        pop = new_pop

    return pop


# ========================
# 输出解的路径拆分
# ========================

def print_solution_detail(ind: Individual,
                          batches: List[Batch],
                          path_lib: Dict[Tuple[str, str], List[Path]],
                          sol_name: str = "Solution",
                          file=None):
    """
    输出个体在各 OD 上的路径拆分结构。
    """

    def out(line: str = ""):
        if file is not None:
            file.write(line + "\n")
        else:
            print(line)

    out(f"\n===== {sol_name} 详细路径拆分 =====")
    for batch in batches:
        od = (batch.origin, batch.destination)
        allocs = ind.od_allocations.get(od, [])
        allocs = [a for a in allocs if a.share > 1e-6]
        if not allocs:
            continue

        paths_for_od = path_lib.get(od, [])
        path_map = {p.path_id: p for p in paths_for_od}

        out(f"\nOD: {batch.origin} -> {batch.destination}, "
            f"Quantity={batch.quantity}, 路径条数={len(allocs)}")
        if len(allocs) == 1:
            out("  是否发生拆分: 否")
        else:
            out("  是否发生拆分: 是")

        for a in allocs:
            p = path_map.get(a.path_id)
            if p is None:
                continue
            node_str = " -> ".join(p.nodes)
            out(f"  share = {a.share:.3f}, path_id = {p.path_id}")
            out(f"    节点序列: {node_str}")
            out(f"    模式序列: {p.modes}")


# ========================
# 主函数: NSGA-II
# ========================

def run_nsga2(filename="data.xlsx",
              pop_size=30,
              generations=10,
              pc=0.9,
              pm=0.3):
    node_names, arcs, timetables, batches = load_network_from_extended(filename)
    path_lib = build_path_library(node_names, arcs, batches, timetables)

    population = build_initial_population(
        batches, path_lib, arcs, timetables,
        big_pool_size=400,
        top_k_cost=80,
        final_pop_size=pop_size
    )

    history_f1_min = []
    history_f1_mean = []
    history_f2_min = []
    history_f3_min = []

    # 不再在这里 repair 初始种群，只在进化过程中处理

    for gen in range(generations):
        for ind in population:
            evaluate_individual(ind, batches, path_lib, arcs, timetables)

        fronts = non_dominated_sort(population)
        ranks = {}
        for i, front in enumerate(fronts):
            for ind in front:
                ranks[ind] = i

        all_distances = {}
        for front in fronts:
            d = crowding_distance(front)
            all_distances.update(d)

        mating_pool: List[Individual] = []
        while len(mating_pool) < pop_size:
            mating_pool.append(tournament_select(population, all_distances, ranks))

        offspring: List[Individual] = []
        while len(offspring) < pop_size:
            if random.random() < pc:
                p1, p2 = random.sample(mating_pool, 2)
                c1, c2 = crossover(p1, p2, batches)
            else:
                p = random.choice(mating_pool)
                c1 = Individual(
                    od_allocations={od: [PathAllocation(a.path_id, a.share)
                                         for a in allocs]
                                    for od, allocs in p.od_allocations.items()}
                )
                c2 = Individual(
                    od_allocations={od: [PathAllocation(a.path_id, a.share)
                                         for a in allocs]
                                    for od, allocs in p.od_allocations.items()}
                )

            if random.random() < pm:
                mutate(c1, batches, path_lib)
            if random.random() < pm:
                mutate(c2, batches, path_lib)

            repair(c1, batches, path_lib, arcs, timetables)
            repair(c2, batches, path_lib, arcs, timetables)

            offspring.append(c1)
            if len(offspring) < pop_size:
                offspring.append(c2)

        union = population + offspring
        for ind in union:
            evaluate_individual(ind, batches, path_lib, arcs, timetables)

        fronts = non_dominated_sort(union)
        new_pop: List[Individual] = []
        for front in fronts:
            if len(new_pop) + len(front) <= pop_size:
                new_pop.extend(front)
            else:
                d = crowding_distance(front)
                sorted_front = sorted(front, key=lambda x: d[x], reverse=True)
                new_pop.extend(sorted_front[:pop_size - len(new_pop)])
                break

        population = new_pop

        f1_vals = [ind.objectives[0] for ind in population]
        f2_vals = [ind.objectives[1] for ind in population]
        f3_vals = [ind.objectives[2] for ind in population]

        f1_min = min(f1_vals)
        f1_mean = float(np.mean(f1_vals))
        f2_min = min(f2_vals)
        f3_min = min(f3_vals)

        history_f1_min.append(f1_min)
        history_f1_mean.append(f1_mean)
        history_f2_min.append(f2_min)
        history_f3_min.append(f3_min)

        print(f"Gen {gen}: f1(min,mean) = {f1_min:.2f}, {f1_mean:.2f}; "
              f"f2(min) = {f2_min:.2f}; f3(min) = {f3_min:.2f}")

    generations_axis = list(range(generations))

    plt.figure()
    plt.plot(generations_axis, history_f1_min, label="f1 min (成本)")
    plt.plot(generations_axis, history_f1_mean, label="f1 mean (成本平均)")
    plt.xlabel("Generation")
    plt.ylabel("Cost")
    plt.title("成本随代数变化")
    plt.legend()
    plt.tight_layout()
    plt.savefig("evolution_f1.png")

    plt.figure()
    plt.plot(generations_axis, history_f2_min, label="f2 min (排放)")
    plt.xlabel("Generation")
    plt.ylabel("Emission")
    plt.title("排放随代数变化")
    plt.legend()
    plt.tight_layout()
    plt.savefig("evolution_f2.png")

    plt.figure()
    plt.plot(generations_axis, history_f3_min, label="f3 min (时间)")
    plt.xlabel("Generation")
    plt.ylabel("Time (h)")
    plt.title("完工时间随代数变化")
    plt.legend()
    plt.tight_layout()
    plt.savefig("evolution_f3.png")

    fronts = non_dominated_sort(population)
    pareto = fronts[0]

    print("\nFinal Pareto front:")
    for i, ind in enumerate(pareto):
        f1, f2, f3 = ind.objectives
        print(f"Sol {i}: cost={f1:.2f}, emission={f2:.2f}, time={f3:.2f}")

    for i, ind in enumerate(pareto[:3]):
        print_solution_detail(ind, batches, path_lib, sol_name=f"Pareto 解 {i}")

    with open("results.txt", "w", encoding="utf-8") as f:
        f.write("Final Pareto front:\n")
        for i, ind in enumerate(pareto):
            f1, f2, f3 = ind.objectives
            line = f"Sol {i}: cost={f1:.2f}, emission={f2:.2f}, time={f3:.2f}"
            f.write(line + "\n")

        for i, ind in enumerate(pareto[:3]):
            print_solution_detail(ind, batches, path_lib,
                                  sol_name=f"Pareto 解 {i}", file=f)

    return pareto, population, path_lib, batches


if __name__ == "__main__":
    pareto, population, path_lib, batches = run_nsga2(
        filename="data.xlsx",  # 你的真实数据文件名
        pop_size=30,
        generations=10,
        pc=0.9,
        pm=0.3,
    )
